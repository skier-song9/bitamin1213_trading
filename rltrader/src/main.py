import os
import sys
import logging
import argparse
import json
import schedule
import time
import keyring
import requests
import pprint
import traceback
# import pykis
import pandas as pd

from quantylab.rltrader import settings
from quantylab.rltrader import utils
from quantylab.rltrader.utils import *
from quantylab.rltrader import data_manager_3
from quantylab.rltrader.environment import Environment
from quantylab.rltrader.agent import Agent

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['train', 'test', 'update', 'predict'], default='train')
    parser.add_argument('--ver', choices=['v1', 'v2', 'v3', 'v4', 'v4.1', 'v4.2'], default='v4.1')
    parser.add_argument('--name', default=utils.get_time_str())
    parser.add_argument('--stock_code', nargs='+')
    parser.add_argument('--rl_method', choices=['dqn', 'pg', 'ac', 'a2c', 'a3c', 'ppo', 'monkey'], default='a2c')
    parser.add_argument('--net', choices=['dnn', 'lstm', 'cnn', 'monkey'], default='dnn')
    parser.add_argument('--backend', choices=['pytorch', 'tensorflow', 'plaidml'], default='pytorch')
    parser.add_argument('--start_date', default='202030560901')
    parser.add_argument('--end_date', default='202405241530')
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--discount_factor', type=float, default=0.99)
    parser.add_argument('--balance', type=int, default=100000000)
    
    ### üì¢Ïã§ÏãúÍ∞Ñ Ìä∏Î†àÏù¥Îî©Ïö© ÌååÎùºÎØ∏ÌÑ∞
    parser.add_argument('--is_start_end', default=0, type=int) #0=ÏõîÏöîÏùº,1=ÎÇòÎ®∏ÏßÄÏöîÏùº,2=Í∏àÏöîÏùº
    parser.add_argument('--is_mock', type=int, default=1) # 1=mock_invest, 0=real_invest

    args = parser.parse_args()

    # ÌïôÏäµÍ∏∞ ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï
    output_name = f'{args.mode}_{args.name}_{args.rl_method}_{args.net}'
    learning = args.mode in ['train', 'update']
    reuse_models = args.mode in ['test', 'update', 'predict']
    value_network_name = f'{args.name}_{args.rl_method}_{args.net}_value.mdl'
    policy_network_name = f'{args.name}_{args.rl_method}_{args.net}_policy.mdl'
    start_epsilon = 1 if args.mode in ['train', 'update'] else 0
    num_epoches = 300 if args.mode in ['train', 'update'] else 1
    num_steps = 120 if args.net in ['lstm', 'cnn'] else 1

    # Backend ÏÑ§Ï†ï
    os.environ['RLTRADER_BACKEND'] = args.backend
    if args.backend == 'tensorflow':
        os.environ['KERAS_BACKEND'] = 'tensorflow'
    elif args.backend == 'plaidml':
        os.environ['KERAS_BACKEND'] = 'plaidml.keras.backend'

    # Ï∂úÎ†• Í≤ΩÎ°ú ÏÉùÏÑ±
    output_path = os.path.join(settings.BASE_DIR, 'output', output_name)
    if not os.path.isdir(output_path):
        os.makedirs(output_path)

    # ÌååÎùºÎØ∏ÌÑ∞ Í∏∞Î°ù
    params = json.dumps(vars(args))
    with open(os.path.join(output_path, 'params.json'), 'w') as f:
        f.write(params)

    # Î™®Îç∏ Í≤ΩÎ°ú Ï§ÄÎπÑ
    # Î™®Îç∏ Ìè¨Î©ßÏùÄ TensorFlowÎäî h5, PyTorchÎäî pickle
    value_network_path = os.path.join(settings.BASE_DIR, 'models', value_network_name)
    policy_network_path = os.path.join(settings.BASE_DIR, 'models', policy_network_name)

    # Î°úÍ∑∏ Í∏∞Î°ù ÏÑ§Ï†ï
    log_path = os.path.join(output_path, f'{output_name}_pred_{get_today_str()}.log') if args.mode == ' predict' else os.path.join(output_path, f'{output_name}.log')
    if os.path.exists(log_path):
        os.remove(log_path)
    logging.basicConfig(format='%(message)s')
    logger = logging.getLogger(settings.LOGGER_NAME)
    logger.setLevel(logging.DEBUG)
    logger.propagate = False
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setLevel(logging.INFO)
    file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    logger.info(params)
    
    # Backend ÏÑ§Ï†ï, Î°úÍ∑∏ ÏÑ§Ï†ïÏùÑ Î®ºÏ†ÄÌïòÍ≥† RLTrader Î™®ÎìàÎì§ÏùÑ Ïù¥ÌõÑÏóê ÏûÑÌè¨Ìä∏Ìï¥Ïïº Ìï®
    from quantylab.rltrader.learners import ReinforcementLearner, DQNLearner, \
        PolicyGradientLearner, ActorCriticLearner, A2CLearner, A3CLearner, PPOLearner

    common_params = {}
    list_stock_code = []
    list_chart_data = []
    list_training_data = []
    list_min_trading_price = []
    list_max_trading_price = []

    ### üì¢ÌïúÍµ≠Ìà¨ÏûêÏ¶ùÍ∂å API ÌôúÏö©ÏùÑ ÏúÑÌïú ÏΩîÎìú [ÏãúÏûë]
    # Ïã§Ï†ÑÍ≥ÑÏ¢å
    api_keys = read_json('./quantylab/api.json')
    investment_type = 'mock_invest' if args.is_mock==1 else 'real_invest'
    api_keys = api_keys[investment_type]
    keyring.set_password('app_key','user',api_keys['app_key'])
    keyring.set_password('app_secret','user',api_keys['app_secret'])
    APP_KEY, APP_SECRET, ACCOUNT = api_keys['app_key'], api_keys['app_secret'], api_keys['account']
    # res = get_access_token(APP_KEY, APP_SECRET, investment_type)
    # pprint.pprint(res.json(), indent=4)
    # ACCESS_TOKEN = res.json()['access_token']
    ACCESS_TOKEN = api_keys['access_token']
    key_info = {
        "appkey": APP_KEY,                  
        "appsecret": APP_SECRET 
    }
    account_info ={
        "account_code": ACCOUNT.split('-')[0],   
        "product_code": ACCOUNT.split('-')[1]
    }
    # if investment_type =='mock_invest':
    #     domain = pykis.DomainInfo(kind="virtual")
    #     api = pykis.Api(key_info=key_info,domain_info=domain,account_info=account_info)
    # else:
    #     api = pykis.Api(key_info=key_info,account_info=account_info)
    ### üì¢ÌïúÍµ≠Ìà¨ÏûêÏ¶ùÍ∂å API ÌôúÏö©ÏùÑ ÏúÑÌïú ÏΩîÎìú [ÎÅù]

    for stock_code in args.stock_code:
        # Ï∞®Ìä∏ Îç∞Ïù¥ÌÑ∞, ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        chart_data, training_data = data_manager_3.load_data(
            stock_code, args.start_date, args.end_date, ver=args.ver)

        assert len(chart_data) >= num_steps
        
        # ÏµúÏÜå/ÏµúÎåÄ Îã®Ïùº Îß§Îß§ Í∏àÏï° ÏÑ§Ï†ï
        min_trading_price = 500
        max_trading_price = 10000000

        # Í≥µÌÜµ ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï
        common_params = {'rl_method': args.rl_method, 
            'net': args.net, 'num_steps': num_steps, 'lr': args.lr,
            'balance': args.balance, 'num_epoches': num_epoches, 
            'discount_factor': args.discount_factor, 'start_epsilon': start_epsilon,
            'output_path': output_path, 'reuse_models': reuse_models}

        # Í∞ïÌôîÌïôÏäµ ÏãúÏûë
        learner = None
        if args.rl_method != 'a3c':
            common_params.update({'stock_code': stock_code,
                'chart_data': chart_data, 
                'training_data': training_data,
                'min_trading_price': min_trading_price, 
                'max_trading_price': max_trading_price})
            if args.rl_method == 'dqn':
                learner = DQNLearner(**{**common_params, 
                    'value_network_path': value_network_path})
            elif args.rl_method == 'pg':
                learner = PolicyGradientLearner(**{**common_params, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'ac':
                learner = ActorCriticLearner(**{**common_params, 
                    'value_network_path': value_network_path, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'a2c':
                learner = A2CLearner(**{**common_params, 
                    'value_network_path': value_network_path, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'ppo':
                learner = PPOLearner(**{**common_params, 
                    'value_network_path': value_network_path, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'monkey':
                common_params['net'] = args.rl_method
                common_params['num_epoches'] = 10
                common_params['start_epsilon'] = 1
                learning = False
                learner = ReinforcementLearner(**common_params)
        else:
            list_stock_code.append(stock_code)
            list_chart_data.append(chart_data)
            list_training_data.append(training_data)
            list_min_trading_price.append(min_trading_price)
            list_max_trading_price.append(max_trading_price)

    if args.rl_method == 'a3c':
        learner = A3CLearner(**{
            **common_params, 
            'list_stock_code': list_stock_code, 
            'list_chart_data': list_chart_data, 
            'list_training_data': list_training_data,
            'list_min_trading_price': list_min_trading_price, 
            'list_max_trading_price': list_max_trading_price,
            'value_network_path': value_network_path, 
            'policy_network_path': policy_network_path})
    
    assert learner is not None

    if args.mode in ['train', 'test', 'update']:
        learner.run(learning=learning)
        if args.mode in ['train', 'update']:
            learner.save_models()


### *‚ö†Ô∏èÏΩîÎìú ÏàòÏ†ï ÏãúÏûë‚ö†Ô∏è*  ###
    elif args.mode == 'predict':
        ### load properties.json
        props = read_json('./quantylab/properties.json')

        ### Load Informations
        '''
        - is_start_end Î≥ÄÏàò : 0Ïù¥Î©¥ ÏõîÏöîÏùº, 1Ïù¥Î©¥ ÎÇòÎ®∏ÏßÄ ÏöîÏùº, 2Ïù¥Î©¥ Í∏àÏöîÏùºÏùÑ ÏùòÎØ∏

        - ÏõîÏöîÏùº : initial_balanceÎ•º APIÎ°ú Î∂àÎü¨ÏôÄÏÑú jsonÏóê ÏóÖÎç∞Ïù¥Ìä∏,
                balanceÎèÑ initial_balanceÎ°ú ÏóÖÎç∞Ïù¥Ìä∏,
                ÎÇòÎ®∏ÏßÄ Ï†ïÎ≥¥Îì§ÏùÄ 0ÏúºÎ°ú Î¶¨ÏÖã
        - Î™®Îì† ÏöîÏùº Í≥µÌÜµ : Î™®Îì† Ï†ïÎ≥¥Îì§ÏùÑ json ÌååÏùºÏóêÏÑú Î∂àÎü¨Ïò¥
        >> 'INITIAL_BALANCE', 'BALANCE', 'PORTFOLIO_VALUE', 'NUM_BUY',
        'NUM_SELL', 'NUM_HOLD', 'RATIO_HOLD', 'PROFITLOSS' Î≥ÄÏàòÎ™ÖÏúºÎ°ú Ìï¥Îãπ Í∞íÎì§ÏùÑ Ï†ÄÏû•Ìï®.
        
        - 
        '''
        if args.is_start_end == 0: ### ÏõîÏöîÏùºÏóêÎßå Ïã§ÌñâÎêòÎäî ÏΩîÎìú
            ### API ÌôúÏö©Ìï¥ÏÑú ÎÇ¥ Í≥ÑÏ¢åÏóêÏÑú ÏûîÏï° Î∂àÎü¨Ïò§Í∏∞
            INITIAL_BALANCE = int(get_balance_api(
                ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type
            ))
            props['initial_balance'] = INITIAL_BALANCE
            props['balance'] = INITIAL_BALANCE
            props['num_stocks'] = 0
            props['portfolio_value'] = INITIAL_BALANCE
            props['num_buy'] = 0
            props['num_sell'] = 0
            props['num_hold'] = 0
            props['ratio_hold'] = 0
            props['profitloss'] = 0
            props['avg_buy_price'] = 0

        # Î™®Îì† ÏöîÏùº Í≥µÌÜµ : Ï†ïÎ≥¥ Î∂àÎü¨Ïò§Í∏∞
        props_list = ['INITIAL_BALANCE', 'BALANCE', 'NUM_STOCKS', 'PORTFOLIO_VALUE', 'NUM_BUY', 'NUM_SELL', 'NUM_HOLD', 'RATIO_HOLD', 'PROFITLOSS', 'AVG_BUY_PRICE']
        for pl in props_list:
            globals()[f'{pl}'] = props[f'{pl.lower()}'] # Î≥ÄÏàò ÎèôÏ†Å Ìï†Îãπ
        
        ### Learner Í∞ùÏ≤¥Ïùò Î≥ÄÏàòÍ∞íÏùÑ ÏàòÏ†ï >> agentÏùò preset() Ìò∏Ï∂ú
        learner.agent.preset(INITIAL_BALANCE, BALANCE, NUM_STOCKS, PORTFOLIO_VALUE, NUM_BUY, NUM_SELL, 
         NUM_HOLD, RATIO_HOLD, PROFITLOSS, AVG_BUY_PRICE)

        ### ÌòÑÏû¨ ÌïúÍµ≠ ÏãúÍ∞ÑÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ïò§Ï†Ñ 8Ïãú 59Î∂Ñ 0.5Ï¥àÏóê while Î£®ÌîÑ ÏãúÏûë
        while True:
            # ÌòÑÏû¨ ÏãúÍ∞ÑÏùÑ Í∞ÄÏ†∏ÏôÄÏÑú ÌïúÍµ≠ ÏãúÍ∞ÑÏúºÎ°ú Î≥ÄÌôò
            current_time = time.localtime()
            if current_time.tm_hour >= 8 and current_time.tm_min >= 59 and current_time.tm_sec >= 0:
                break
            time.sleep(0.5)

        ### scheduling ÏÑ§Ï†ï
        results = [] # ÌïòÎ£ªÎèôÏïàÏùò Í≤∞Í≥ºÎì§ÏùÑ Ï†ÄÏû•Ìï† Î≥ÄÏàò

        ### 1Î∂ÑÎßàÎã§ predictÎ•º Ìò∏Ï∂ú
        counter = 0 
        pt = time.time()
        try:
            while True:
                while True:
                    ct = time.time()
                    ctl = time.localtime()
                    if counter == 0:
                        break
                    if (ct-pt) >= 60:
                        pt = ct
                        break
                    continue
                counter += 1
                if int(get_dtime_str()) >= 152000:
                    break 
                print(f"{counter} | {ctl.tm_hour}:{ctl.tm_min}:{ctl.tm_sec} | predict started")
                result = learner.predict()

                ### predictÏùò resultÎ°ú Îß§Ïàò/Îß§ÎèÑ/Í¥ÄÎßù ÌñâÎèô ÌåêÎã® ÌõÑ ÏàòÌñâ
                pred_value, pred_policy = result[0][-2],result[0][-1] 
                action, confidence, exploration = learner.agent.decide_action(
                    pred_value, pred_policy, 0 # epsilonÏùÄ 0ÏúºÎ°ú Ìï¥Ïïº Î™®ÌóòÏùÑ Ïïà Ìï®.
                )
                print(f'action:{action}, confidence:{confidence}')
                
                ### Îß§ÎèÑ/Îß§Ïàò API ÏΩîÎìú - agentÏùò act()Ìï®ÏàòÎ•º predictÏóê ÎßûÍ≤å Íµ¨ÌòÑ
                '''
                - ÌïúÍµ≠Ìà¨ÏûêÏ¶ùÍ∂åÏùÄ Îß§Ïàò Í∏àÏï°Ïóê Îî∞Î•∏ Ï∞®Îì±ÏàòÏàòÎ£å >> validate_action ÎåÄÏã† pvalidate_action()Ìï®Ïàò ÏÇ¨Ïö©
                - decide_trading_unit ÌõÑÏóê chargeÎäî utilsÏùò get_charge()Î°ú Î∂àÎü¨Ïò®Îã§. 
                '''
                if not learner.agent.pvalidate_action(action):
                    action = learner.agent.ACTION_HOLD
                curr_price = learner.environment.get_price()
                do_nothing = False
                if action == learner.agent.ACTION_BUY:
                    # Îß§ÏàòÌï† Îã®ÏúÑÎ•º ÌåêÎã®
                    trading_unit = learner.agent.decide_trading_unit(confidence)
                    # if counter == 1:
                    # recent_volume =  int(int(learner.environment.observation.iloc[5]) * 2.5)
                    # trading_unit = min(trading_unit, recent_volume)
                    '''
                    while True:
                        1. Îß§ÏàòÍ∞ÄÎä•Ï°∞Ìöå(get_possible)Ìï¥ÏÑú Îß§ÏàòÍ∞ÄÎä•ÏàòÎüâ Ï°∞Ìöå >> min(trading_unit, possible)
                        2. ÏãúÏû•Í∞ÄÎ°ú trading_unitÎßåÌÅº Îß§Ïàò(buy_stock)
                        3. time.sleep(2.5) : 2.5Ï¥à ÎèôÏïà Í∏∞Îã§Î¶∞Îã§.
                        4. Îß§ÏàòÍ∞Ä Ï≤¥Í≤∞ÎêòÏóàÎäîÏßÄ ÌôïÏù∏(select_order) > order_price, is_buyed
                            5. is_buyedÍ∞Ä FalseÏù¥Î©¥ ÎØ∏Ï≤¥Í≤∞Îêú Í≤É >> order_id Ï£ºÎ¨∏ÏùÑ Ï∑®ÏÜåÌïòÍ≥† Îã§Ïãú 1Î≤àÏúºÎ°ú ÎèåÏïÑÍ∞ê
                        6. is_buyedÍ∞Ä TrueÎùºÎ©¥ Ï†ïÎ≥¥Îì§ ÏóÖÎç∞Ïù¥Ìä∏ & break
                    '''
                    buy_counter = 0
                    while 1:
                        possible_unit = get_possible(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code)
                        time.sleep(1)
                        trading_unit = min(trading_unit, int(possible_unit))

                        order_id, order_time = buy_stock(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code,trading_unit)
                        print(f"{order_time}, {order_id} Îß§Ïàò Ï£ºÎ¨∏, Ï£ºÎ¨∏ÏàòÎüâ: {trading_unit}")
                        time.sleep(5)

                        order_price, is_buyed = select_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code, order_id)
                        time.sleep(1)
                        if not is_buyed:
                            ### Ï£ºÎ¨∏ Ï∑®ÏÜå
                            rt_cd, order_time = cancel_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,order_id)
                            time.sleep(1)
                            print(f"{order_time}, {order_id} Îß§Ïàò Ï£ºÎ¨∏Ïù¥ 5Ï¥àÎÇ¥Ïóê Ï≤¥Í≤∞ÎêòÏßÄ ÏïäÏïÑ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.")
                            buy_counter+=1
                            if buy_counter == 3:
                                # Îçî Ïù¥ÏÉÅ Ï£ºÎ¨∏ ÏãúÎèÑ Î©àÏ∂îÍ≥† actionÏùÑ HOLDÎ°ú Î∞îÍøà.
                                order_price, trading_unit = 0,0
                                learner.agent.action = learner.agent.ACTION_HOLD
                                learner.agent.num_hold += 1
                                break
                            continue

                        ### Ï£ºÎ¨∏Ïù¥ Ï≤¥Í≤∞Îêú Í≤ΩÏö∞
                        # ÏûîÏï° ÏóÖÎç∞Ïù¥Ìä∏
                        balance = int(get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type))
                        time.sleep(1)
                        # Îß§Ïàò ÏÑ±Í≥µ Ïãú, ÏàòÏàòÎ£åÎ•º Ï†ÅÏö©ÌïòÏó¨ Ï¥ù Îß§Ïàò Í∏àÏï° ÏÇ∞Ï†ï Î∞è Î≥ÄÏàò ÏóÖÎç∞Ïù¥Ìä∏
                        order_price, trading_unit = int(order_price), int(trading_unit)
                        hantu_charge, add_price = get_charge(order_price, trading_unit)
                        invest_amount = order_price * (1 + hantu_charge) * trading_unit + add_price
                        if invest_amount > 0:
                            learner.agent.avg_buy_price = \
                                (learner.agent.avg_buy_price * learner.agent.num_stocks + order_price * trading_unit) \
                                    / (learner.agent.num_stocks + trading_unit)  # Ï£ºÎãπ Îß§Ïàò Îã®Í∞Ä Í∞±Ïã†
                            learner.agent.balance = balance  # Î≥¥Ïú† ÌòÑÍ∏àÏùÑ Í∞±Ïã†
                            learner.agent.num_stocks += trading_unit  # Î≥¥Ïú† Ï£ºÏãù ÏàòÎ•º Í∞±Ïã†
                            learner.agent.num_buy += 1  # Îß§Ïàò ÌöüÏàò Ï¶ùÍ∞Ä
                        break
                    # end inner Îß§Ïàò while 
                                            

                elif action == learner.agent.ACTION_SELL:

                    ### Îß§ÎèÑ Loop
                    buy_counter = 0
                    while 1:
                        trading_unit = learner.agent.decide_trading_unit(confidence)
                        trading_unit = min(trading_unit, learner.agent.num_stocks)
                        
                        order_id, order_time = sell_stock(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code,trading_unit)
                        print(f"{order_time}, {order_id} Îß§ÎèÑ Ï£ºÎ¨∏, Ï£ºÎ¨∏ÏàòÎüâ: {trading_unit}")
                    #     print(get_time_str())
                        time.sleep(15)
                    #     print(get_time_str())
                        order_price, is_buyed = select_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code, order_id)
                        time.sleep(1)
                        if not is_buyed:
                            ### Ï£ºÎ¨∏ Ï∑®ÏÜå
                            rt_cd, order_time = cancel_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,order_id)
                            time.sleep(1)
                            print(f"{order_time}, {order_id} Îß§ÎèÑ Ï£ºÎ¨∏Ïù¥ 5Ï¥àÎÇ¥Ïóê Ï≤¥Í≤∞ÎêòÏßÄ ÏïäÏïÑ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.")
                            buy_counter+=1
                            if buy_counter == 2:
                                # Îçî Ïù¥ÏÉÅ Ï£ºÎ¨∏ ÏãúÎèÑ Î©àÏ∂îÍ≥† actionÏùÑ HOLDÎ°ú Î∞îÍøà.
                                order_price, trading_unit = 0,0
                                learner.agent.action = learner.agent.ACTION_HOLD
                                learner.agent.num_hold += 1
                                break
                            continue

                        ### Îß§ÎèÑ Ï£ºÎ¨∏Ïù¥ Ï≤¥Í≤∞Îêú Í≤ΩÏö∞
                        # ÏûîÏï° ÏóÖÎç∞Ïù¥Ìä∏
                        balance = int(get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type))
                        time.sleep(1)
                        # Îß§ÎèÑ ÏÑ±Í≥µ Ïãú, ÏàòÏàòÎ£åÎ•º Ï†ÅÏö©ÌïòÏó¨ Ï¥ù Îß§ÎèÑ Í∏àÏï° ÏÇ∞Ï†ï Î∞è Î≥ÄÏàò ÏóÖÎç∞Ïù¥Ìä∏
                        order_price, trading_unit = int(order_price), int(trading_unit)
                        invest_amount = order_price * (1+ learner.agent.HANTU_TAX) * trading_unit
                        if invest_amount > 0:
                            learner.agent.avg_buy_price = \
                                (learner.agent.avg_buy_price * learner.agent.num_stocks - order_price * trading_unit) \
                                    / (learner.agent.num_stocks - trading_unit) \
                                        if learner.agent.num_stocks > trading_unit else 0  # Ï£ºÎãπ Îß§ÎèÑ Îã®Í∞Ä Í∞±Ïã†
                            learner.agent.balance = balance  # Î≥¥Ïú† ÌòÑÍ∏àÏùÑ Í∞±Ïã†
                            learner.agent.num_stocks -= trading_unit  # Î≥¥Ïú† Ï£ºÏãù ÏàòÎ•º Í∞±Ïã†
                            learner.agent.num_sell += 1  # Îß§ÎèÑ ÌöüÏàò Ï¶ùÍ∞Ä
                        break
                    # end inner Îß§ÎèÑ while 

                    ### üì¢Îß§ÎèÑ API Ìò∏Ï∂ú
                    # Îß§ÎèÑ Í∞ÄÎä• Ïó¨Î∂ÄÎ•º ÌôïÏù∏ÌïòÍ≥† attempt_to_sell Ìï®Ïàò Ìò∏Ï∂ú
                    # if learner.agent.num_stocks >= trading_unit:
                    #     attempt_to_sell(api, learner, stock_code, trading_unit, curr_price)
                    # else:
                    #     learner.agent.num_hold += 1  # Î≥¥Ïú† Ï£ºÏãù ÏàòÍ∞Ä Îß§ÎèÑ Îã®ÏúÑÎ≥¥Îã§ Ï†ÅÏùÑ Í≤ΩÏö∞
                    #     print("Not enough stocks to sell")

                else:
                    order_price, trading_unit = 0,0
                    learner.agent.num_hold += 1
                

                '''
                num_stocks, num_buy, num_sell, num_hold, 
                portfolio_value, ratio_hold, profitloss, avg_buy_price Í≥ÑÏÇ∞ ÌõÑ 
                jsonÌååÏùºÏóê Ï†ÄÏû• 
                '''
                ### Í≥ÑÏÇ∞.
                if counter == 1:
                    continue
                invest_amount = 0 if invest_amount else invest_amount
                learner.agent.portfolio_value = int(get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type)) + invest_amount
                time.sleep(1)
                learner.agent.profitloss = learner.agent.portfolio_value / learner.agent.initial_balance - 1
                learner.agent.ratio_hold = learner.agent.num_stocks * curr_price \
                    / learner.agent.portfolio_value
                
                ### Ïó¨Í∏∞ÏÑú Î∂ÑÎ¥â Îç∞Ïù¥ÌÑ∞Î•º 1Í∞ú Î∂àÎü¨ÏôÄÏÑú chart_dataÏùò ÎßàÏßÄÎßâ rowÎ°ú Ï∂îÍ∞Ä.
                time.sleep(1)
                stck_cntg_hour,stck_oprc,stck_hgpr,stck_lwpr,stck_prpr,cntg_vol = get_min_data(APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code)
                time.sleep(1)
                t = time.localtime()
                stck_cntg_hour = f"{t.tm_year}{t.tm_mon:02}{t.tm_mday:02}{stck_cntg_hour[:4]}"
                new_row = pd.DataFrame(
                    [[stck_cntg_hour,stck_oprc,stck_hgpr,stck_lwpr,stck_prpr,cntg_vol]],
                    columns=['date','open','high','low','close','volume']
                )
                chart_data = pd.concat([chart_data,new_row]).reset_index(drop=True)
                # chart_data.to_csv(f"../data/v1/{stock_code}.csv", index=0)
                new_pre_data = data_manager_3.preprocess(chart_data.iloc[-120:,:6].reset_index(drop=True))
                new_tr = new_pre_data[data_manager_3.COLUMNS_TRAINING_DATA_V1].iloc[-1]
                new_tr = pd.DataFrame([new_tr])
                training_data = pd.concat([training_data, new_tr]).reset_index(drop=True)
                # update learner's data
                learner.chart_data = chart_data
                learner.training_data = training_data
                learner.environment.chart_data = chart_data
                learner.agent.environment = learner.environment 

                ### is_start_end Í∞íÏù¥ 2Ïùº Îïå (Í∏àÏöîÏùº)
                # counter 390Î≤àÏùº Îïå -> ÎßàÏßÄÎßâ Ïã§Ìñâ Ïãú Ï£ºÏãùÏùÑ Ïó¨Ï†ÑÌûà Î≥¥Ïú†ÌïòÍ≥† ÏûàÏúºÎ©¥ Î™®Îëê Îß§ÎèÑÌïòÎäî ÏΩîÎìú
                if (args.is_start_end ==2) and (int(get_dtime_str()) >= 152800):
                    # Î™®Îëê Îß§ÎèÑ
                    buy_counter = 0
                    while 1:
                        trading_unit = learner.agent.num_stocks                        
                        order_id, order_time = sell_stock(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code,trading_unit)
                        time.sleep(1)
                        print(f"{order_time}, {order_id} Îß§ÎèÑ Ï£ºÎ¨∏")
                    #     print(get_time_str())
                        time.sleep(15)
                    #     print(get_time_str())
                        order_price, is_buyed = select_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code, order_id)
                        if not is_buyed:
                            ### Ï£ºÎ¨∏ Ï∑®ÏÜå
                            rt_cd, order_time = cancel_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,order_id)
                            time.sleep(1)
                            print(f"{order_time}, {order_id} Îß§ÎèÑ Ï£ºÎ¨∏Ïù¥ 5Ï¥àÎÇ¥Ïóê Ï≤¥Í≤∞ÎêòÏßÄ ÏïäÏïÑ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.")
                            buy_counter+=1
                            if buy_counter == 2:
                                print(get_time_str(),'ÏûîÎüâ Îß§ÎèÑ Ïã§Ìå®')
                                break
                            continue

                        ### Îß§ÎèÑ Ï£ºÎ¨∏Ïù¥ Ï≤¥Í≤∞Îêú Í≤ΩÏö∞
                        # ÏûîÏï° ÏóÖÎç∞Ïù¥Ìä∏
                        balance = int(get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type))
                        time.sleep(1)
                        # Îß§ÎèÑ ÏÑ±Í≥µ Ïãú, ÏàòÏàòÎ£åÎ•º Ï†ÅÏö©ÌïòÏó¨ Ï¥ù Îß§ÎèÑ Í∏àÏï° ÏÇ∞Ï†ï Î∞è Î≥ÄÏàò ÏóÖÎç∞Ïù¥Ìä∏
                        order_price, trading_unit = int(order_price), int(trading_unit)
                        income = order_price * (1+ learner.agent.HANTU_TAX) * trading_unit
                        if invest_amount > 0:
                            learner.agent.avg_buy_price =  0  # Ï£ºÎãπ Îß§ÎèÑ Îã®Í∞Ä Í∞±Ïã†
                            learner.agent.balance = balance  # Î≥¥Ïú† ÌòÑÍ∏àÏùÑ Í∞±Ïã†
                            learner.agent.num_stocks -= trading_unit  # Î≥¥Ïú† Ï£ºÏãù ÏàòÎ•º Í∞±Ïã†
                            learner.agent.num_sell += 1  # Îß§ÎèÑ ÌöüÏàò Ï¶ùÍ∞Ä
                        break
                # end Í∏àÏöîÏùº process
                    
                # Record Log
                try:
                    if order_price is None and trading_unit is None:
                        order_price = 0 
                        trading_unit = 0 
                    acts = {0:'BUY', 1:'SELL',2:'HOLD'}
                    logger.debug(
                        f"c{counter} | #TIME:{get_dtime_str(ct)} | #ACTION:{acts[action]} | #CONFIDENCE:{confidence} | #PRICE:{'HOLD' if action == 2 else order_price} | #UNIT:{'HOLD' if action == 2 else trading_unit} | #PV:{learner.agent.portfolio_value} | #STOCKS:{learner.agent.num_stocks} | #LOSS:{learner.agent.profitloss}"
                    )
                except:
                    traceback.print_exc()

            # end while

            ### Ïó¨Í∏∞ÏÑú Î∂ÑÎ¥â Îç∞Ïù¥ÌÑ∞Î•º Ìïú Î≤à Îçî Î∂àÎü¨Ïò¥ >> 15Ïãú30Î∂ÑÍ∫º
            time.sleep(1)
            stck_cntg_hour,stck_oprc,stck_hgpr,stck_lwpr,stck_prpr,cntg_vol = get_min_data(APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code)
            stck_cntg_hour = '152000'
            time.sleep(1)
            t = time.localtime()
            stck_cntg_hour = f"{t.tm_year}{t.tm_mon:02}{t.tm_mday:02}{stck_cntg_hour[:4]}"
            new_row = pd.DataFrame(
                [[stck_cntg_hour,stck_oprc,stck_hgpr,stck_lwpr,stck_prpr,cntg_vol]],
                columns=['date','open','high','low','close','volume']
            )
            chart_data = pd.concat([chart_data,new_row]).reset_index(drop=True)
            # chart_data.to_csv(f"../data/v1/{stock_code}.csv", index=0)
            new_pre_data = data_manager_3.preprocess(chart_data.iloc[-120:,:6].reset_index(drop=True))
            new_tr = new_pre_data[data_manager_3.COLUMNS_TRAINING_DATA_V1].iloc[-1]
            new_tr = pd.DataFrame([new_tr])
            training_data = pd.concat([training_data, new_tr]).reset_index(drop=True)
            # update learner's data
            learner.chart_data = chart_data
            learner.training_data = training_data
            learner.environment.chart_data = chart_data
            learner.agent.environment = learner.environment 

        # end try
        except:
            traceback.print_exc()
        finally:
            # chart_data Ï†ÄÏû•
            cd = pd.read_csv(f'../data/v1/{stock_code}.csv')
            last_idx = int(chart_data[chart_data['date']==args.end_date].index[-1])+1
            # print('last_idx', last_idx)
            cd = pd.concat([cd, chart_data.iloc[last_idx:,:]]).reset_index(drop=True)
            cd.to_csv(f'../data/v1/{stock_code}.csv',index=0)

            ### jsonÏóê Ï†ÄÏû•
            datas = {
                "initial_balance" : learner.agent.initial_balance, 
                "balance" : learner.agent.balance,
                "num_stocks" : learner.agent.num_stocks,
                "portfolio_value" : learner.agent.portfolio_value,
                "num_buy" : learner.agent.num_buy,
                "num_sell" : learner.agent.num_sell,
                "num_hold" : learner.agent.num_hold, 
                "ratio_hold" : learner.agent.ratio_hold,  
                "profitloss" : learner.agent.profitloss,
                "avg_buy_price" : learner.agent.avg_buy_price
                }
            write_json(data=datas,filename='./quantylab/properties.json')
            # schedule.clear()
            # pass

            ## access_token ÌèêÍ∏∞
            try:
                host = "https://openapivts.koreainvestment.com:29443" if investment_type=='mock_inverst' else 'https://openapi.koreainvestment.com:9443'
                endpoint = "/oauth2/revokeP"
                url = host+endpoint
                headers = {"content-type":"application/json; charset=UTF-8"}
                body = {"appkey":APP_KEY, 
                        "appsecret":APP_SECRET,
                        "token":ACCESS_TOKEN
                        }
                resp = requests.post(url, headers=headers, data=json.dumps(body))
                if resp.json()['code'] == 200:
                    print("access token successfully revoked.")
                else:
                    pprint.pprint(resp.json()['message'])
            except:
                print("access token already revoked.")

            print("‚úÖfinish",get_time_str())
        
        
### *‚ö†Ô∏èÏΩîÎìú ÏàòÏ†ï ÎÅù‚ö†Ô∏è*  ###