import os
import sys
import logging
import argparse
import json
import schedule
import time
import keyring
import requests
import pprint
# import pykis
import pandas as pd

from quantylab.rltrader import settings
from quantylab.rltrader import utils
from quantylab.rltrader.utils import *
from quantylab.rltrader import data_manager_3
from quantylab.rltrader.environment import Environment
from quantylab.rltrader.agent import Agent

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['train', 'test', 'update', 'predict'], default='train')
    parser.add_argument('--ver', choices=['v1', 'v2', 'v3', 'v4', 'v4.1', 'v4.2'], default='v4.1')
    parser.add_argument('--name', default=utils.get_time_str())
    parser.add_argument('--stock_code', nargs='+')
    parser.add_argument('--rl_method', choices=['dqn', 'pg', 'ac', 'a2c', 'a3c', 'ppo', 'monkey'], default='a2c')
    parser.add_argument('--net', choices=['dnn', 'lstm', 'cnn', 'monkey'], default='dnn')
    parser.add_argument('--backend', choices=['pytorch', 'tensorflow', 'plaidml'], default='pytorch')
    parser.add_argument('--start_date', default='202030560901')
    parser.add_argument('--end_date', default='202405241530')
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--discount_factor', type=float, default=0.99)
    parser.add_argument('--balance', type=int, default=100000000)
    
    ### ğŸ“¢ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”©ìš© íŒŒë¼ë¯¸í„°
    parser.add_argument('--is_start_end', default=0, type=int) #0=ì›”ìš”ì¼,1=ë‚˜ë¨¸ì§€ìš”ì¼,2=ê¸ˆìš”ì¼
    parser.add_argument('--is_mock', type=int, default=1) # 1=mock_invest, 0=real_invest

    args = parser.parse_args()

    # í•™ìŠµê¸° íŒŒë¼ë¯¸í„° ì„¤ì •
    output_name = f'{args.mode}_{args.name}_{args.rl_method}_{args.net}'
    learning = args.mode in ['train', 'update']
    reuse_models = args.mode in ['test', 'update', 'predict']
    value_network_name = f'{args.name}_{args.rl_method}_{args.net}_value.mdl'
    policy_network_name = f'{args.name}_{args.rl_method}_{args.net}_policy.mdl'
    start_epsilon = 1 if args.mode in ['train', 'update'] else 0
    num_epoches = 500 if args.mode in ['train', 'update'] else 1
    num_steps = 5 if args.net in ['lstm', 'cnn'] else 1

    # Backend ì„¤ì •
    os.environ['RLTRADER_BACKEND'] = args.backend
    if args.backend == 'tensorflow':
        os.environ['KERAS_BACKEND'] = 'tensorflow'
    elif args.backend == 'plaidml':
        os.environ['KERAS_BACKEND'] = 'plaidml.keras.backend'

    # ì¶œë ¥ ê²½ë¡œ ìƒì„±
    output_path = os.path.join(settings.BASE_DIR, 'output', output_name)
    if not os.path.isdir(output_path):
        os.makedirs(output_path)

    # íŒŒë¼ë¯¸í„° ê¸°ë¡
    params = json.dumps(vars(args))
    with open(os.path.join(output_path, 'params.json'), 'w') as f:
        f.write(params)

    # ëª¨ë¸ ê²½ë¡œ ì¤€ë¹„
    # ëª¨ë¸ í¬ë©§ì€ TensorFlowëŠ” h5, PyTorchëŠ” pickle
    value_network_path = os.path.join(settings.BASE_DIR, 'models', value_network_name)
    policy_network_path = os.path.join(settings.BASE_DIR, 'models', policy_network_name)

    # ë¡œê·¸ ê¸°ë¡ ì„¤ì •
    log_path = os.path.join(output_path, f'{output_name}.log')
    if os.path.exists(log_path):
        os.remove(log_path)
    logging.basicConfig(format='%(message)s')
    logger = logging.getLogger(settings.LOGGER_NAME)
    logger.setLevel(logging.DEBUG)
    logger.propagate = False
    stream_handler = logging.StreamHandler(sys.stdout)
    stream_handler.setLevel(logging.INFO)
    file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    logger.info(params)
    
    # Backend ì„¤ì •, ë¡œê·¸ ì„¤ì •ì„ ë¨¼ì €í•˜ê³  RLTrader ëª¨ë“ˆë“¤ì„ ì´í›„ì— ì„í¬íŠ¸í•´ì•¼ í•¨
    from quantylab.rltrader.learners import ReinforcementLearner, DQNLearner, \
        PolicyGradientLearner, ActorCriticLearner, A2CLearner, A3CLearner, PPOLearner

    common_params = {}
    list_stock_code = []
    list_chart_data = []
    list_training_data = []
    list_min_trading_price = []
    list_max_trading_price = []

    ### ğŸ“¢í•œêµ­íˆ¬ìì¦ê¶Œ API í™œìš©ì„ ìœ„í•œ ì½”ë“œ [ì‹œì‘]
    # ì‹¤ì „ê³„ì¢Œ
    api_keys = read_json('./quantylab/api.json')
    investment_type = 'mock_invest' if args.is_mock==1 else 'real_invest'
    api_keys = api_keys[investment_type]
    keyring.set_password('app_key','user',api_keys['app_key'])
    keyring.set_password('app_secret','user',api_keys['app_secret'])
    APP_KEY, APP_SECRET, ACCOUNT = api_keys['app_key'], api_keys['app_secret'], api_keys['account']
    res = get_access_token(APP_KEY, APP_SECRET, investment_type)
    pprint.pprint(res.json(), indent=4)
    ACCESS_TOKEN = res.json()['access_token']
    key_info = {
        "appkey": APP_KEY,                  
        "appsecret": APP_SECRET 
    }
    account_info ={
        "account_code": ACCOUNT.split('-')[0],   
        "product_code": ACCOUNT.split('-')[1]
    }
    # if investment_type =='mock_invest':
    #     domain = pykis.DomainInfo(kind="virtual")
    #     api = pykis.Api(key_info=key_info,domain_info=domain,account_info=account_info)
    # else:
    #     api = pykis.Api(key_info=key_info,account_info=account_info)
    ### ğŸ“¢í•œêµ­íˆ¬ìì¦ê¶Œ API í™œìš©ì„ ìœ„í•œ ì½”ë“œ [ë]

    for stock_code in args.stock_code:
        # ì°¨íŠ¸ ë°ì´í„°, í•™ìŠµ ë°ì´í„° ì¤€ë¹„
        chart_data, training_data = data_manager_3.load_data(
            stock_code, args.start_date, args.end_date, ver=args.ver)

        assert len(chart_data) >= num_steps
        
        # ìµœì†Œ/ìµœëŒ€ ë‹¨ì¼ ë§¤ë§¤ ê¸ˆì•¡ ì„¤ì •
        min_trading_price = 1000
        max_trading_price = 1000000000

        # ê³µí†µ íŒŒë¼ë¯¸í„° ì„¤ì •
        common_params = {'rl_method': args.rl_method, 
            'net': args.net, 'num_steps': num_steps, 'lr': args.lr,
            'balance': args.balance, 'num_epoches': num_epoches, 
            'discount_factor': args.discount_factor, 'start_epsilon': start_epsilon,
            'output_path': output_path, 'reuse_models': reuse_models}

        # ê°•í™”í•™ìŠµ ì‹œì‘
        learner = None
        if args.rl_method != 'a3c':
            common_params.update({'stock_code': stock_code,
                'chart_data': chart_data, 
                'training_data': training_data,
                'min_trading_price': min_trading_price, 
                'max_trading_price': max_trading_price})
            if args.rl_method == 'dqn':
                learner = DQNLearner(**{**common_params, 
                    'value_network_path': value_network_path})
            elif args.rl_method == 'pg':
                learner = PolicyGradientLearner(**{**common_params, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'ac':
                learner = ActorCriticLearner(**{**common_params, 
                    'value_network_path': value_network_path, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'a2c':
                learner = A2CLearner(**{**common_params, 
                    'value_network_path': value_network_path, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'ppo':
                learner = PPOLearner(**{**common_params, 
                    'value_network_path': value_network_path, 
                    'policy_network_path': policy_network_path})
            elif args.rl_method == 'monkey':
                common_params['net'] = args.rl_method
                common_params['num_epoches'] = 10
                common_params['start_epsilon'] = 1
                learning = False
                learner = ReinforcementLearner(**common_params)
        else:
            list_stock_code.append(stock_code)
            list_chart_data.append(chart_data)
            list_training_data.append(training_data)
            list_min_trading_price.append(min_trading_price)
            list_max_trading_price.append(max_trading_price)

    if args.rl_method == 'a3c':
        learner = A3CLearner(**{
            **common_params, 
            'list_stock_code': list_stock_code, 
            'list_chart_data': list_chart_data, 
            'list_training_data': list_training_data,
            'list_min_trading_price': list_min_trading_price, 
            'list_max_trading_price': list_max_trading_price,
            'value_network_path': value_network_path, 
            'policy_network_path': policy_network_path})
    
    assert learner is not None

    if args.mode in ['train', 'test', 'update']:
        learner.run(learning=learning)
        if args.mode in ['train', 'update']:
            learner.save_models()


### *âš ï¸ì½”ë“œ ìˆ˜ì • ì‹œì‘âš ï¸*  ###
    elif args.mode == 'predict':
        ### load properties.json
        props = read_json('./properties.json')

        ### Load Informations
        '''
        - is_start_end ë³€ìˆ˜ : 0ì´ë©´ ì›”ìš”ì¼, 1ì´ë©´ ë‚˜ë¨¸ì§€ ìš”ì¼, 2ì´ë©´ ê¸ˆìš”ì¼ì„ ì˜ë¯¸

        - ì›”ìš”ì¼ : initial_balanceë¥¼ APIë¡œ ë¶ˆëŸ¬ì™€ì„œ jsonì— ì—…ë°ì´íŠ¸,
                balanceë„ initial_balanceë¡œ ì—…ë°ì´íŠ¸,
                ë‚˜ë¨¸ì§€ ì •ë³´ë“¤ì€ 0ìœ¼ë¡œ ë¦¬ì…‹
        - ëª¨ë“  ìš”ì¼ ê³µí†µ : ëª¨ë“  ì •ë³´ë“¤ì„ json íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜´
        >> 'INITIAL_BALANCE', 'BALANCE', 'PORTFOLIO_VALUE', 'NUM_BUY',
        'NUM_SELL', 'NUM_HOLD', 'RATIO_HOLD', 'PROFITLOSS' ë³€ìˆ˜ëª…ìœ¼ë¡œ í•´ë‹¹ ê°’ë“¤ì„ ì €ì¥í•¨.
        
        - 
        '''
        if args.is_start_end == 0: ### ì›”ìš”ì¼ì—ë§Œ ì‹¤í–‰ë˜ëŠ” ì½”ë“œ
            ### API í™œìš©í•´ì„œ ë‚´ ê³„ì¢Œì—ì„œ ì”ì•¡ ë¶ˆëŸ¬ì˜¤ê¸°
            INITIAL_BALANCE = get_balance_api(
                ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type
            )
            props['initial_balance'] = INITIAL_BALANCE
            props['balance'] = INITIAL_BALANCE
            props['num_stocks'] = 0
            props['portfolio_value'] = 0
            props['num_buy'] = 0
            props['num_sell'] = 0
            props['num_hold'] = 0
            props['ratio_hold'] = 0
            props['profitloss'] = 0
            props['avg_buy_price'] = 0

        # ëª¨ë“  ìš”ì¼ ê³µí†µ : ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
        props_list = ['INITIAL_BALANCE', 'BALANCE', 'NUM_STOCKS', 'PORTFOLIO_VALUE', 'NUM_BUY', 'NUM_SELL', 'NUM_HOLD', 'RATIO_HOLD', 'PROFITLOSS', 'AVG_BUY_PRICE']
        for pl in props_list:
            globals()[f'{pl}'] = props[f'{pl.lower()}'] # ë³€ìˆ˜ ë™ì  í• ë‹¹
        
        ### Learner ê°ì²´ì˜ ë³€ìˆ˜ê°’ì„ ìˆ˜ì • >> agentì˜ preset() í˜¸ì¶œ
        learner.agent.preset(INITIAL_BALANCE, BALANCE, NUM_STOCKS, PORTFOLIO_VALUE, NUM_BUY, NUM_SELL, 
         NUM_HOLD, RATIO_HOLD, PROFITLOSS, AVG_BUY_PRICE)

        ### í˜„ì¬ í•œêµ­ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ì „ 8ì‹œ 59ë¶„ 0.5ì´ˆì— while ë£¨í”„ ì‹œì‘
        while True:
            # í˜„ì¬ ì‹œê°„ì„ ê°€ì ¸ì™€ì„œ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜
            current_time = time.localtime()
            if current_time.tm_hour == 8 and current_time.tm_min == 59 and current_time.tm_sec >= 0:
                break
            time.sleep(0.5)

        ### scheduling ì„¤ì •
        results = [] # í•˜ë£»ë™ì•ˆì˜ ê²°ê³¼ë“¤ì„ ì €ì¥í•  ë³€ìˆ˜

        ### 1ë¶„ë§ˆë‹¤ predictë¥¼ í˜¸ì¶œ
        counter = 0 # í•˜ë£¨ì— 390ë²ˆ predictë¥¼ ì‹¤í–‰ (ì˜¤ì „9ì‹œ~ì˜¤í›„3ì‹œ30 : 390ë¶„)
        pt = time.time()
        try:
            while True:
                while True:
                    ct = time.time()
                    if (ct-pt) >= 60:
                        pt = ct
                        break
                    continue
                if int(get_dtime_str()) >= 153030:
                    break 
                print(f"{counter} | {ct.tm_hour}:{ct.tm_min}:{ct.tm_sec} | predict started")
                result = learner.predict()

                ### predictì˜ resultë¡œ ë§¤ìˆ˜/ë§¤ë„/ê´€ë§ í–‰ë™ íŒë‹¨ í›„ ìˆ˜í–‰
                pred_value, pred_policy = result[-2],result[-1] 
                action, confidence, exploration = learner.agent.decide_action(
                    pred_value, pred_policy, 0 # epsilonì€ 0ìœ¼ë¡œ í•´ì•¼ ëª¨í—˜ì„ ì•ˆ í•¨.
                )
                
                ### ë§¤ë„/ë§¤ìˆ˜ API ì½”ë“œ - agentì˜ act()í•¨ìˆ˜ë¥¼ predictì— ë§ê²Œ êµ¬í˜„
                '''
                - í•œêµ­íˆ¬ìì¦ê¶Œì€ ë§¤ìˆ˜ ê¸ˆì•¡ì— ë”°ë¥¸ ì°¨ë“±ìˆ˜ìˆ˜ë£Œ >> validate_action ëŒ€ì‹  pvalidate_action()í•¨ìˆ˜ ì‚¬ìš©
                - decide_trading_unit í›„ì— chargeëŠ” utilsì˜ get_charge()ë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤. 
                '''
                if not learner.agent.pvalidate_action(action):
                    action = learner.agent.ACTION_HOLD
                curr_price = learner.environment.get_price()
                do_nothing = False
                if action == learner.agent.ACTION_BUY:
                    # ë§¤ìˆ˜í•  ë‹¨ìœ„ë¥¼ íŒë‹¨
                    trading_unit = learner.agent.decide_trading_unit(confidence)
                    '''
                    while True:
                        1. ë§¤ìˆ˜ê°€ëŠ¥ì¡°íšŒ(get_possible)í•´ì„œ ë§¤ìˆ˜ê°€ëŠ¥ìˆ˜ëŸ‰ ì¡°íšŒ >> min(trading_unit, possible)
                        2. ì‹œì¥ê°€ë¡œ trading_unitë§Œí¼ ë§¤ìˆ˜(buy_stock)
                        3. time.sleep(2.5) : 2.5ì´ˆ ë™ì•ˆ ê¸°ë‹¤ë¦°ë‹¤.
                        4. ë§¤ìˆ˜ê°€ ì²´ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸(select_order) > order_price, is_buyed
                            5. is_buyedê°€ Falseì´ë©´ ë¯¸ì²´ê²°ëœ ê²ƒ >> order_id ì£¼ë¬¸ì„ ì·¨ì†Œí•˜ê³  ë‹¤ì‹œ 1ë²ˆìœ¼ë¡œ ëŒì•„ê°
                        6. is_buyedê°€ Trueë¼ë©´ ì •ë³´ë“¤ ì—…ë°ì´íŠ¸ & break
                    '''
                    buy_counter = 0
                    while 1:
                        possible_unit = get_possible(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code)
                        trading_unit = min(trading_unit, int(possible_unit))

                        order_id, order_time = buy_stock(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code,trading_unit)
                        print(f"{order_time}, {order_id} ë§¤ìˆ˜ ì£¼ë¬¸")
                        time.sleep(5)

                        order_price, is_buyed = select_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code, order_id)
                        if not is_buyed:
                            ### ì£¼ë¬¸ ì·¨ì†Œ
                            rt_cd, order_time = cancel_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,order_id)
                            print(f"{order_time}, {order_id} ë§¤ìˆ˜ ì£¼ë¬¸ì´ 5ì´ˆë‚´ì— ì²´ê²°ë˜ì§€ ì•Šì•„ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            buy_counter+=1
                            if buy_counter == 3:
                                # ë” ì´ìƒ ì£¼ë¬¸ ì‹œë„ ë©ˆì¶”ê³  actionì„ HOLDë¡œ ë°”ê¿ˆ.
                                learner.agent.action = learner.agent.ACTION_HOLD
                                learner.agent.num_hold += 1
                                break
                            continue

                        ### ì£¼ë¬¸ì´ ì²´ê²°ëœ ê²½ìš°
                        # ì”ì•¡ ì—…ë°ì´íŠ¸
                        balance = get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type)
                        # ë§¤ìˆ˜ ì„±ê³µ ì‹œ, ìˆ˜ìˆ˜ë£Œë¥¼ ì ìš©í•˜ì—¬ ì´ ë§¤ìˆ˜ ê¸ˆì•¡ ì‚°ì • ë° ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                        order_price, trading_unit = int(order_price), int(trading_unit)
                        hantu_charge, add_price = get_charge(order_price, trading_unit)
                        invest_amount = order_price * (1 + hantu_charge) * trading_unit + add_price
                        if invest_amount > 0:
                            learner.agent.avg_buy_price = \
                                (learner.agent.avg_buy_price * learner.agent.num_stocks + order_price * trading_unit) \
                                    / (learner.agent.num_stocks + trading_unit)  # ì£¼ë‹¹ ë§¤ìˆ˜ ë‹¨ê°€ ê°±ì‹ 
                            learner.agent.balance = balance  # ë³´ìœ  í˜„ê¸ˆì„ ê°±ì‹ 
                            learner.agent.num_stocks += trading_unit  # ë³´ìœ  ì£¼ì‹ ìˆ˜ë¥¼ ê°±ì‹ 
                            learner.agent.num_buy += 1  # ë§¤ìˆ˜ íšŸìˆ˜ ì¦ê°€
                        break
                    # end inner ë§¤ìˆ˜ while 
                                            

                elif action == learner.agent.ACTION_SELL:
                    trading_unit = learner.agent.decide_trading_unit(confidence)
                    trading_unit = min(trading_unit, learner.agent.num_stocks)

                    ### ë§¤ë„ Loop
                    buy_counter = 0
                    while 1:
                        trading_unit = learner.agent.decide_trading_unit(confidence)
                        trading_unit = min(trading_unit, learner.agent.num_stocks)
                        
                        order_id, order_time = sell_stock(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code,trading_unit)
                        print(f"{order_time}, {order_id} ë§¤ë„ ì£¼ë¬¸")
                    #     print(get_time_str())
                        time.sleep(5)
                    #     print(get_time_str())
                        order_price, is_buyed = select_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code, order_id)
                        if not is_buyed:
                            ### ì£¼ë¬¸ ì·¨ì†Œ
                            rt_cd, order_time = cancel_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,order_id)
                            print(f"{order_time}, {order_id} ë§¤ë„ ì£¼ë¬¸ì´ 5ì´ˆë‚´ì— ì²´ê²°ë˜ì§€ ì•Šì•„ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            buy_counter+=1
                            if buy_counter == 3:
                                # ë” ì´ìƒ ì£¼ë¬¸ ì‹œë„ ë©ˆì¶”ê³  actionì„ HOLDë¡œ ë°”ê¿ˆ.
                                learner.agent.action = learner.agent.ACTION_HOLD
                                learner.agent.num_hold += 1
                                break
                            continue

                        ### ë§¤ë„ ì£¼ë¬¸ì´ ì²´ê²°ëœ ê²½ìš°
                        # ì”ì•¡ ì—…ë°ì´íŠ¸
                        balance = get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type)
                        # ë§¤ë„ ì„±ê³µ ì‹œ, ìˆ˜ìˆ˜ë£Œë¥¼ ì ìš©í•˜ì—¬ ì´ ë§¤ë„ ê¸ˆì•¡ ì‚°ì • ë° ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                        order_price, trading_unit = int(order_price), int(trading_unit)
                        income = order_price * (1+ learner.agent.HANTU_TAX) * trading_unit
                        if invest_amount > 0:
                            learner.agent.avg_buy_price = \
                                (learner.agent.avg_buy_price * learner.agent.num_stocks - order_price * trading_unit) \
                                    / (learner.agent.num_stocks - trading_unit) \
                                        if learner.agent.num_stocks > trading_unit else 0  # ì£¼ë‹¹ ë§¤ë„ ë‹¨ê°€ ê°±ì‹ 
                            learner.agent.balance = balance  # ë³´ìœ  í˜„ê¸ˆì„ ê°±ì‹ 
                            learner.agent.num_stocks -= trading_unit  # ë³´ìœ  ì£¼ì‹ ìˆ˜ë¥¼ ê°±ì‹ 
                            learner.agent.num_sell += 1  # ë§¤ë„ íšŸìˆ˜ ì¦ê°€
                        break
                    # end inner ë§¤ë„ while 

                    ### ğŸ“¢ë§¤ë„ API í˜¸ì¶œ
                    # ë§¤ë„ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  attempt_to_sell í•¨ìˆ˜ í˜¸ì¶œ
                    # if learner.agent.num_stocks >= trading_unit:
                    #     attempt_to_sell(api, learner, stock_code, trading_unit, curr_price)
                    # else:
                    #     learner.agent.num_hold += 1  # ë³´ìœ  ì£¼ì‹ ìˆ˜ê°€ ë§¤ë„ ë‹¨ìœ„ë³´ë‹¤ ì ì„ ê²½ìš°
                    #     print("Not enough stocks to sell")

                else:
                    learner.agent.num_hold += 1
                

                '''
                num_stocks, num_buy, num_sell, num_hold, 
                portfolio_value, ratio_hold, profitloss, avg_buy_price ê³„ì‚° í›„ 
                jsoníŒŒì¼ì— ì €ì¥ 
                '''
                ### ê³„ì‚°.
                learner.agent.portfolio_value = get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type)
                learner.agent.profitloss = learner.agent.portfolio_value / learner.agent.initial_balance - 1
                learner.agent.ratio_hold = learner.agent.num_stocks * curr_price \
                    / learner.agent.portfolio_value
                
                # 


                ### ì—¬ê¸°ì„œ ë¶„ë´‰ ë°ì´í„°ë¥¼ 1ê°œ ë¶ˆëŸ¬ì™€ì„œ chart_dataì˜ ë§ˆì§€ë§‰ rowë¡œ ì¶”ê°€.
                stck_cntg_hour,stck_oprc,stck_hgpr,stck_lwpr,stck_prpr,cntg_vol = get_min_data(APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code)
                t = time.localtime()
                stck_cntg_hour = f"{t.tm_year}{t.tm_mon:02}{t.tm_mday:02}{stck_cntg_hour[:4]}"
                new_row = pd.DataFrame(
                    [[stck_cntg_hour,stck_oprc,stck_hgpr,stck_lwpr,stck_prpr,cntg_vol]],
                    columns=['date','open','high','low','close','volume']
                )
                chart_data = pd.concat([chart_data,new_row]).reset_index(drop=True)
                # chart_data.to_csv(f"../data/v1/{stock_code}.csv", index=0)
                new_pre_data = data_manager_3.preprocess(chart_data.iloc[-120:,:6]).reset_index(drop=True)
                new_tr = new_pre_data[data_manager_3.COLUMNS_TRAINING_DATA_V1].iloc[-1]
                new_tr = pd.DataFrame([new_tr])
                training_data = pd.concat([training_data, new_tr]).reset_index(drop=True)
                # update learner's data
                learner.chart_data = chart_data
                learner.training_data = training_data
                learner.environment.chart_data = chart_data
                learner.agent.environment = learner.environment 

                ### is_start_end ê°’ì´ 2ì¼ ë•Œ (ê¸ˆìš”ì¼)
                # counter 390ë²ˆì¼ ë•Œ -> ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œ ì£¼ì‹ì„ ì—¬ì „íˆ ë³´ìœ í•˜ê³  ìˆìœ¼ë©´ ëª¨ë‘ ë§¤ë„í•˜ëŠ” ì½”ë“œ
                if (args.is_start_end ==2) and (int(get_dtime_str()) >= 152800):
                    # ëª¨ë‘ ë§¤ë„
                    buy_counter = 0
                    while 1:
                        trading_unit = learner.agent.num_stocks                        
                        order_id, order_time = sell_stock(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code,trading_unit)
                        print(f"{order_time}, {order_id} ë§¤ë„ ì£¼ë¬¸")
                    #     print(get_time_str())
                        time.sleep(5)
                    #     print(get_time_str())
                        order_price, is_buyed = select_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,stock_code, order_id)
                        if not is_buyed:
                            ### ì£¼ë¬¸ ì·¨ì†Œ
                            rt_cd, order_time = cancel_order(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type,order_id)
                            print(f"{order_time}, {order_id} ë§¤ë„ ì£¼ë¬¸ì´ 5ì´ˆë‚´ì— ì²´ê²°ë˜ì§€ ì•Šì•„ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            buy_counter+=1
                            if buy_counter == 3:
                                print(get_time_str(),'ì”ëŸ‰ ë§¤ë„ ì‹¤íŒ¨')
                                break
                            continue

                        ### ë§¤ë„ ì£¼ë¬¸ì´ ì²´ê²°ëœ ê²½ìš°
                        # ì”ì•¡ ì—…ë°ì´íŠ¸
                        balance = get_balance_api(ACCOUNT,APP_KEY,APP_SECRET,ACCESS_TOKEN,investment_type)
                        # ë§¤ë„ ì„±ê³µ ì‹œ, ìˆ˜ìˆ˜ë£Œë¥¼ ì ìš©í•˜ì—¬ ì´ ë§¤ë„ ê¸ˆì•¡ ì‚°ì • ë° ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                        order_price, trading_unit = int(order_price), int(trading_unit)
                        income = order_price * (1+ learner.agent.HANTU_TAX) * trading_unit
                        if invest_amount > 0:
                            learner.agent.avg_buy_price =  0  # ì£¼ë‹¹ ë§¤ë„ ë‹¨ê°€ ê°±ì‹ 
                            learner.agent.balance = balance  # ë³´ìœ  í˜„ê¸ˆì„ ê°±ì‹ 
                            learner.agent.num_stocks -= trading_unit  # ë³´ìœ  ì£¼ì‹ ìˆ˜ë¥¼ ê°±ì‹ 
                            learner.agent.num_sell += 1  # ë§¤ë„ íšŸìˆ˜ ì¦ê°€
                        break
                    pass

            # end while
            
            
        finally:
            # chart_data ì €ì¥

            ### jsonì— ì €ì¥
            datas = {
                "initial_balance" : learner.agent.initial_balance, 
                "balance" : learner.agent.balance,
                "num_stocks" : learner.agent.num_stocks,
                "portfolio_value" : learner.agent.portfolio_value,
                "num_buy" : learner.agent.num_buy,
                "num_sell" : learner.agent.num_sell,
                "num_hold" : learner.agent.num_hold, 
                "ratio_hold" : learner.agent.ratio_hold,  
                "profitloss" : learner.agent.profitloss,
                "avg_buy_price" : learner.agent.avg_buy_price
                }
            write_json(data=datas,filename='./quantylab/properties.json')
            # schedule.clear()
            pass
        
        
### *âš ï¸ì½”ë“œ ìˆ˜ì • ëâš ï¸*  ###