import os
import logging
import abc
import collections
import threading
import time
import json
import numpy as np
from tqdm import tqdm
from quantylab.rltrader.environment import Environment
from quantylab.rltrader.agent import Agent
from quantylab.rltrader.networks import Network, DNN, LSTMNetwork, CNN
from quantylab.rltrader.visualizer import Visualizer
from quantylab.rltrader import utils
from quantylab.rltrader import settings


logger = logging.getLogger(settings.LOGGER_NAME)


class ReinforcementLearner:
    __metaclass__ = abc.ABCMeta
    lock = threading.Lock()

    def __init__(self, rl_method='rl', stock_code=None, 
                chart_data=None, training_data=None,
                min_trading_price=100000, max_trading_price=10000000, 
                net='dnn', num_steps=1, lr=0.0005, 
                discount_factor=0.9, num_epoches=1000,
                balance=100000000, start_epsilon=1,
                value_network=None, policy_network=None,
                value_network_activation='linear', policy_network_activation='sigmoid',
                output_path='', reuse_models=True, gen_output=True):
        # ì¸ì í™•ì¸ (assertëŠ” í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°˜í™˜í•œë‹¤.)
        assert min_trading_price > 0
        assert max_trading_price > 0
        assert max_trading_price >= min_trading_price
        assert num_steps > 0
        assert lr > 0
        # ê°•í™”í•™ìŠµ ì„¤ì •
        self.rl_method = rl_method
        self.discount_factor = discount_factor
        self.num_epoches = num_epoches
        self.start_epsilon = start_epsilon
        # í™˜ê²½ ì„¤ì •
        self.stock_code = stock_code
        self.chart_data = chart_data
        self.environment = Environment(chart_data)
        # ì—ì´ì „íŠ¸ ì„¤ì •
        self.agent = Agent(self.environment, balance, min_trading_price, max_trading_price, stock_code)
        # í•™ìŠµ ë°ì´í„°
        self.training_data = training_data
        self.sample = None
        self.training_data_idx = -1
        # ë²¡í„° í¬ê¸° = í•™ìŠµ ë°ì´í„° ë²¡í„° í¬ê¸° + ì—ì´ì „íŠ¸ ìƒíƒœ í¬ê¸°
        self.num_features = self.agent.STATE_DIM
        if self.training_data is not None:
            self.num_features += self.training_data.shape[1]
        # ì‹ ê²½ë§ ì„¤ì •
        self.net = net
        self.num_steps = num_steps
        self.lr = lr
        self.value_network = value_network
        self.policy_network = policy_network
        self.reuse_models = reuse_models
        self.value_network_activation = value_network_activation
        self.policy_network_activation = policy_network_activation
        # ê°€ì‹œí™” ëª¨ë“ˆ
        self.visualizer = Visualizer()
        # ë©”ëª¨ë¦¬
        self.memory_sample = []
        self.memory_action = []
        self.memory_reward = []
        self.memory_value = []
        self.memory_policy = []
        self.memory_pv = []
        self.memory_num_stocks = []
        self.memory_exp_idx = []
        # ì—í¬í¬ ê´€ë ¨ ì •ë³´
        self.loss = 0.
        self.itr_cnt = 0
        self.exploration_cnt = 0
        self.batch_size = 0
        # ë¡œê·¸ ë“± ì¶œë ¥ ê²½ë¡œ
        self.output_path = output_path
        self.gen_output = gen_output

    def init_value_network(self, shared_network=None, loss='mse'):
        if self.net == 'dnn':
            self.value_network = DNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, shared_network=shared_network,
                activation=self.value_network_activation, loss=loss)
        elif self.net == 'lstm':
            self.value_network = LSTMNetwork(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=self.value_network_activation, loss=loss)
        elif self.net == 'cnn':
            self.value_network = CNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=self.value_network_activation, loss=loss)
        if self.reuse_models and os.path.exists(self.value_network_path):
            self.value_network.load_model(model_path=self.value_network_path)

    def init_policy_network(self, shared_network=None, loss='binary_crossentropy'):
        if self.net == 'dnn':
            self.policy_network = DNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, shared_network=shared_network,
                activation=self.policy_network_activation, loss=loss)
        elif self.net == 'lstm':
            self.policy_network = LSTMNetwork(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=self.policy_network_activation, loss=loss)
        elif self.net == 'cnn':
            self.policy_network = CNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=self.policy_network_activation, loss=loss)
        if self.reuse_models and os.path.exists(self.policy_network_path):
            self.policy_network.load_model(model_path=self.policy_network_path)

    def reset(self):
        self.sample = None
        self.training_data_idx = -1
        # í™˜ê²½ ì´ˆê¸°í™”
        self.environment.reset()
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.agent.reset()
        # ê°€ì‹œí™” ì´ˆê¸°í™”
        self.visualizer.clear([0, len(self.chart_data)])
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory_sample = []
        self.memory_action = []
        self.memory_reward = []
        self.memory_value = []
        self.memory_policy = []
        self.memory_pv = []
        self.memory_num_stocks = []
        self.memory_exp_idx = []
        # ì—í¬í¬ ê´€ë ¨ ì •ë³´ ì´ˆê¸°í™”
        self.loss = 0.
        self.itr_cnt = 0
        self.exploration_cnt = 0
        self.batch_size = 0

    def build_sample(self):
        self.environment.observe()
        if len(self.training_data) > self.training_data_idx + 1:
            self.training_data_idx += 1
            self.sample = self.training_data[self.training_data_idx].tolist()
            self.sample.extend(self.agent.get_states())
            return self.sample
        return None

    @abc.abstractmethod
    def get_batch(self):
        pass

    def fit(self):
        # ë°°ì¹˜ í•™ìŠµ ë°ì´í„° ìƒì„±
        x, y_value, y_policy = self.get_batch()
        # ì†ì‹¤ ì´ˆê¸°í™”
        self.loss = None
        if len(x) > 0:
            loss = 0
            if y_value is not None:
                # ê°€ì¹˜ ì‹ ê²½ë§ ê°±ì‹ 
                loss += self.value_network.train_on_batch(x, y_value)
            if y_policy is not None:
                # ì •ì±… ì‹ ê²½ë§ ê°±ì‹ 
                loss += self.policy_network.train_on_batch(x, y_policy)
            self.loss = loss

    def visualize(self, epoch_str, num_epoches, epsilon):
        self.memory_action = [Agent.ACTION_HOLD] * (self.num_steps - 1) + self.memory_action
        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks
        if self.value_network is not None:
            self.memory_value = [np.array([np.nan] * len(Agent.ACTIONS))] \
                                * (self.num_steps - 1) + self.memory_value
        if self.policy_network is not None:
            self.memory_policy = [np.array([np.nan] * len(Agent.ACTIONS))] \
                                * (self.num_steps - 1) + self.memory_policy
        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv
        self.visualizer.plot(
            epoch_str=epoch_str, num_epoches=num_epoches, 
            epsilon=epsilon, action_list=Agent.ACTIONS, 
            actions=self.memory_action, 
            num_stocks=self.memory_num_stocks, 
            outvals_value=self.memory_value, 
            outvals_policy=self.memory_policy,
            exps=self.memory_exp_idx, 
            initial_balance=self.agent.initial_balance, 
            pvs=self.memory_pv,
        )
        self.visualizer.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))

    def run(self, learning=True):
        info = (
            f'[{self.stock_code}] RL:{self.rl_method} NET:{self.net} '
            f'LR:{self.lr} DF:{self.discount_factor} '
        )
        with self.lock:
            logger.debug(info)

        # ì‹œì‘ ì‹œê°„
        time_start = time.time()

        # ê°€ì‹œí™” ì¤€ë¹„
        # ì°¨íŠ¸ ë°ì´í„°ëŠ” ë³€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¯¸ë¦¬ ê°€ì‹œí™”
        self.visualizer.prepare(self.environment.chart_data, info)

        # ê°€ì‹œí™” ê²°ê³¼ ì €ì¥í•  í´ë” ì¤€ë¹„
        if self.gen_output:
            self.epoch_summary_dir = os.path.join(self.output_path, f'epoch_summary_{self.stock_code}')
            if not os.path.isdir(self.epoch_summary_dir):
                os.makedirs(self.epoch_summary_dir)
            else:
                for f in os.listdir(self.epoch_summary_dir):
                    os.remove(os.path.join(self.epoch_summary_dir, f))

        # í•™ìŠµì— ëŒ€í•œ ì •ë³´ ì´ˆê¸°í™”
        max_portfolio_value = 0
        epoch_win_cnt = 0

        # ì—í¬í¬ ë°˜ë³µ
        for epoch in tqdm(range(self.num_epoches)):
            time_start_epoch = time.time()

            # step ìƒ˜í”Œì„ ë§Œë“¤ê¸° ìœ„í•œ í
            q_sample = collections.deque(maxlen=self.num_steps)
            
            # í™˜ê²½, ì—ì´ì „íŠ¸, ì‹ ê²½ë§, ê°€ì‹œí™”, ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            self.reset()

            # í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ë¡ íƒí—˜ ë¹„ìœ¨ ê°ì†Œ
            if learning:
                epsilon = self.start_epsilon * (1 - (epoch / (self.num_epoches - 1)))
            else:
                epsilon = self.start_epsilon

            for i in tqdm(range(len(self.training_data)), leave=False):
                # ìƒ˜í”Œ ìƒì„±
                next_sample = self.build_sample()
                if next_sample is None:
                    break

                # num_stepsë§Œí¼ ìƒ˜í”Œ ì €ì¥
                q_sample.append(next_sample)
                if len(q_sample) < self.num_steps:
                    continue

                # ê°€ì¹˜, ì •ì±… ì‹ ê²½ë§ ì˜ˆì¸¡
                pred_value = None
                pred_policy = None
                if self.value_network is not None:
                    pred_value = self.value_network.predict(list(q_sample))
                if self.policy_network is not None:
                    pred_policy = self.policy_network.predict(list(q_sample))
                
                # ì‹ ê²½ë§ ë˜ëŠ” íƒí—˜ì— ì˜í•œ í–‰ë™ ê²°ì •
                action, confidence, exploration = \
                    self.agent.decide_action(pred_value, pred_policy, epsilon)

                # ê²°ì •í•œ í–‰ë™ì„ ìˆ˜í–‰í•˜ê³  ë³´ìƒ íšë“
                reward = self.agent.act(action, confidence)

                # í–‰ë™ ë° í–‰ë™ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ê¸°ì–µ
                self.memory_sample.append(list(q_sample))
                self.memory_action.append(action)
                self.memory_reward.append(reward)
                if self.value_network is not None:
                    self.memory_value.append(pred_value)
                if self.policy_network is not None:
                    self.memory_policy.append(pred_policy)
                self.memory_pv.append(self.agent.portfolio_value)
                self.memory_num_stocks.append(self.agent.num_stocks)
                if exploration:
                    self.memory_exp_idx.append(self.training_data_idx)

                # ë°˜ë³µì— ëŒ€í•œ ì •ë³´ ê°±ì‹ 
                self.batch_size += 1
                self.itr_cnt += 1
                self.exploration_cnt += 1 if exploration else 0

            # ì—í¬í¬ ì¢…ë£Œ í›„ í•™ìŠµ
            if learning:
                self.fit()

            # ì—í¬í¬ ê´€ë ¨ ì •ë³´ ë¡œê·¸ ê¸°ë¡
            num_epoches_digit = len(str(self.num_epoches))
            epoch_str = str(epoch + 1).rjust(num_epoches_digit, '0')
            time_end_epoch = time.time()
            elapsed_time_epoch = time_end_epoch - time_start_epoch
            logger.debug(f'[{self.stock_code}][Epoch {epoch_str}/{self.num_epoches}] '
                f'Epsilon:{epsilon:.4f} #Expl.:{self.exploration_cnt}/{self.itr_cnt} '
                f'#Buy:{self.agent.num_buy} #Sell:{self.agent.num_sell} #Hold:{self.agent.num_hold} '
                f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '
                f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}')

            # ì—í¬í¬ ê´€ë ¨ ì •ë³´ ê°€ì‹œí™”
            if self.gen_output:
                if self.num_epoches == 1 or (epoch + 1) % max(int(self.num_epoches / 10), 1) == 0:
                    self.visualize(epoch_str, self.num_epoches, epsilon)

            # í•™ìŠµ ê´€ë ¨ ì •ë³´ ê°±ì‹ 
            max_portfolio_value = max(
                max_portfolio_value, self.agent.portfolio_value)
            if self.agent.portfolio_value > self.agent.initial_balance:
                epoch_win_cnt += 1

        # ì¢…ë£Œ ì‹œê°„
        time_end = time.time()
        elapsed_time = time_end - time_start

        # í•™ìŠµ ê´€ë ¨ ì •ë³´ ë¡œê·¸ ê¸°ë¡
        with self.lock:
            logger.debug(f'[{self.stock_code}] Elapsed Time:{elapsed_time:.4f} '
                f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}')

    def save_models(self):
        if self.value_network is not None and self.value_network_path is not None:
            self.value_network.save_model(self.value_network_path)
        if self.policy_network is not None and self.policy_network_path is not None:
            self.policy_network.save_model(self.policy_network_path)

    ### ğŸ“¢ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© ì‹œ predict() í•¨ìˆ˜ ì‹¤í–‰
    def predict(self):
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        # self.agent.reset()

        # step ìƒ˜í”Œì„ ë§Œë“¤ê¸° ìœ„í•œ í
        q_sample = collections.deque(maxlen=self.num_steps)
        
        result = []
        while True:
            # ìƒ˜í”Œ ìƒì„±
            next_sample = self.build_sample() # observationì´ ìµœì‹ ìœ¼ë¡œ ê°±ì‹ ë¨.
            if next_sample is None:
                break

            # num_stepsë§Œí¼ ìƒ˜í”Œ ì €ì¥
            q_sample.append(next_sample)
            if len(q_sample) < self.num_steps:
                continue

            # ê°€ì¹˜, ì •ì±… ì‹ ê²½ë§ ì˜ˆì¸¡
            pred_value = None
            pred_policy = None
            if self.value_network is not None:
                pred_value = self.value_network.predict(list(q_sample)).tolist()
            if self.policy_network is not None:
                pred_policy = self.policy_network.predict(list(q_sample)).tolist()
            
            # ì‹ ê²½ë§ì— ì˜í•œ í–‰ë™ ê²°ì •
            result.append((self.environment.observation[0], pred_value, pred_policy))

        if self.gen_output:
            with open(os.path.join(self.output_path, f'pred_{self.stock_code}.json'), 'w') as f:
                print(json.dumps(result), file=f)

        return result


class DQNLearner(ReinforcementLearner):
    def __init__(self, *args, value_network_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.value_network_path = value_network_path
        self.init_value_network()

    def get_batch(self):
        memory = zip(
            reversed(self.memory_sample),
            reversed(self.memory_action),
            reversed(self.memory_value),
            reversed(self.memory_reward),
        )
        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))
        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        value_max_next = 0
        for i, (sample, action, value, reward) in enumerate(memory):
            x[i] = sample
            r = self.memory_reward[-1] - reward
            y_value[i] = value
            y_value[i, action] = r + self.discount_factor * value_max_next
            value_max_next = value.max()
        return x, y_value, None


class PolicyGradientLearner(ReinforcementLearner):
    def __init__(self, *args, policy_network_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.policy_network_path = policy_network_path
        self.init_policy_network()

    def get_batch(self):
        memory = zip(
            reversed(self.memory_sample),
            reversed(self.memory_action),
            reversed(self.memory_policy),
            reversed(self.memory_reward),
        )
        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))
        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        for i, (sample, action, policy, reward) in enumerate(memory):
            x[i] = sample
            r = self.memory_reward[-1] - reward
            y_policy[i, :] = policy
            y_policy[i, action] = utils.sigmoid(r)
        return x, None, y_policy


class ActorCriticLearner(ReinforcementLearner):
    def __init__(self, *args, shared_network=None, 
        value_network_path=None, policy_network_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        if shared_network is None:
            self.shared_network = Network.get_shared_network(
                net=self.net, num_steps=self.num_steps, 
                input_dim=self.num_features,
                output_dim=self.agent.NUM_ACTIONS)
        else:
            self.shared_network = shared_network
        self.value_network_path = value_network_path
        self.policy_network_path = policy_network_path
        if self.value_network is None:
            self.init_value_network(shared_network=self.shared_network)
        if self.policy_network is None:
            self.init_policy_network(shared_network=self.shared_network)

    def get_batch(self):
        memory = zip(
            reversed(self.memory_sample),
            reversed(self.memory_action),
            reversed(self.memory_value),
            reversed(self.memory_policy),
            reversed(self.memory_reward),
        )
        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))
        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        value_max_next = 0
        for i, (sample, action, value, policy, reward) in enumerate(memory):
            x[i] = sample
            r = self.memory_reward[-1] - reward
            y_value[i, :] = value
            y_value[i, action] = r + self.discount_factor * value_max_next
            y_policy[i, :] = policy
            y_policy[i, action] = utils.sigmoid(r)
            value_max_next = value.max()
        return x, y_value, y_policy


class A2CLearner(ActorCriticLearner):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
    def get_batch(self):
        memory = zip(
            reversed(self.memory_sample),
            reversed(self.memory_action),
            reversed(self.memory_value),
            reversed(self.memory_policy),
            reversed(self.memory_reward),
        )
        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))
        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        value_max_next = 0
        reward_next = self.memory_reward[-1]
        for i, (sample, action, value, policy, reward) in enumerate(memory):
            x[i] = sample
            r = reward_next + self.memory_reward[-1] - reward * 2
            reward_next = reward
            y_value[i, :] = value
            y_value[i, action] = np.tanh(r + self.discount_factor * value_max_next)
            advantage = y_value[i, action] - y_value[i].mean()
            y_policy[i, :] = policy
            y_policy[i, action] = utils.sigmoid(advantage)
            value_max_next = value.max()
        return x, y_value, y_policy
    

class A3CLearner(ReinforcementLearner):
    def __init__(self, *args, list_stock_code=None, 
        list_chart_data=None, list_training_data=None,
        list_min_trading_price=None, list_max_trading_price=None, 
        value_network_path=None, policy_network_path=None,
        **kwargs):
        assert len(list_training_data) > 0
        super().__init__(*args, **kwargs)
        self.num_features += list_training_data[0].shape[1]

        # ê³µìœ  ì‹ ê²½ë§ ìƒì„±
        self.shared_network = Network.get_shared_network(
            net=self.net, num_steps=self.num_steps, 
            input_dim=self.num_features,
            output_dim=self.agent.NUM_ACTIONS)
        self.value_network_path = value_network_path
        self.policy_network_path = policy_network_path
        if self.value_network is None:
            self.init_value_network(shared_network=self.shared_network)
        if self.policy_network is None:
            self.init_policy_network(shared_network=self.shared_network)

        # A2CLearner ìƒì„±
        self.learners = []
        for (stock_code, chart_data, training_data, 
            min_trading_price, max_trading_price) in zip(
                list_stock_code, list_chart_data, list_training_data,
                list_min_trading_price, list_max_trading_price
            ):
            learner = A2CLearner(*args, 
                stock_code=stock_code, chart_data=chart_data, 
                training_data=training_data,
                min_trading_price=min_trading_price, 
                max_trading_price=max_trading_price, 
                shared_network=self.shared_network,
                value_network=self.value_network,
                policy_network=self.policy_network, **kwargs)
            self.learners.append(learner)

    def run(self, learning=True):
        threads = []
        for learner in self.learners:
            threads.append(threading.Thread(
                target=learner.run, daemon=True, kwargs={'learning': learning}
            ))
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()

    def predict(self):
        threads = []
        for learner in self.learners:
            threads.append(threading.Thread(
                target=learner.predict, daemon=True
            ))
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()


class PPOLearner(A2CLearner):
    def __init__(self, *args, lmb=0.95, eps=0.1, K=3, **kwargs):
        kwargs['value_network_activation'] = 'tanh'
        kwargs['policy_network_activation'] = 'tanh'
        super().__init__(*args, **kwargs)
        self.lmb = lmb
        self.eps = eps
        self.K = K
        
    def get_batch(self):
        memory = zip(
            reversed(self.memory_sample),
            reversed(self.memory_action),
            reversed(self.memory_value),
            reversed(self.memory_policy),
            reversed(self.memory_reward),
        )
        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))
        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))
        value_max_next = 0
        reward_next = self.memory_reward[-1]
        for i, (sample, action, value, policy, reward) in enumerate(memory):
            x[i] = sample
            # r = reward_next - reward
            # reward_next = reward
            y_value[i, :] = value
            y_value[i, action] = np.tanh(reward + self.discount_factor * value_max_next)
            advantage = y_value[i, action] - y_value[i].mean()
            y_policy[i, :] = policy
            y_policy[i, action] = advantage
            value_max_next = value.max()
        return x, y_value, y_policy
    
    def fit(self):
        # ë°°ì¹˜ í•™ìŠµ ë°ì´í„° ìƒì„±
        x, y_value, y_policy = self.get_batch()
        # ì†ì‹¤ ì´ˆê¸°í™”
        self.loss = None
        if len(x) > 0:
            loss = 0
            if y_value is not None:
                # ê°€ì¹˜ ì‹ ê²½ë§ ê°±ì‹ 
                loss += self.value_network.train_on_batch(x, y_value)
            if y_policy is not None:
                # ì •ì±… ì‹ ê²½ë§ ê°±ì‹ 
                loss += self.policy_network.train_on_batch_for_ppo(x, y_policy, list(reversed(self.memory_action)), self.eps, self.K)
            self.loss = loss
