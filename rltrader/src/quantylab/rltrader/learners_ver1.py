class ReinforcementLearner: #ì´ í´ë˜ìŠ¤ëŠ” DQN, PolicyGradient, ActorCritic, A2CLearner í´ë˜ìŠ¤ê°€ ìƒì†í•˜ëŠ” ìƒìœ„ í´ë˜ìŠ¤ì„. -> í™˜ê²½, ì—ì´ì „ì¸ , ì‹ ê²½ë§ ì¸ìŠ¤í„´ìŠ¤ë“¤ê³¼ í•™ìŠµ ë°ì´í„°ë¥¼ ì†ì„±ìœ¼ë¡œ ê°€ì§
    __metaclass__ = abc.ABCMeta
    lock = threading.Lock()

    def __init__(self, rl_method='rl', stock_code=None,  #rl_methodëŠ” ì–´ë–¤ ê°•í™”í•™ìŠµ ì“°ëƒì— ë”°ë¼ ë‹¬ë¼ì§(DQN -> dqn, ac, a2c, a3c)
                ###ğŸ“¢chart_data, training_data ê³ ë ¤
                chart_data=None,
                training_data=None,
                #chart_dataëŠ” ì£¼ì‹ ì¼ë´‰ ì°¨íŠ¸ ë°ì´í„°, training_dataëŠ” í•™ìŠµì„ ìœ„í•œ ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°
                min_trading_price=100000, max_trading_price=10000000, 
                net='dnn', num_steps=1, lr=0.0005, # netì¸ìëŠ” dnn, lstm, cnn ë“±ì˜ ê°’ì´ ë  ìˆ˜ ìˆìœ¼ë©° ì´ ê°’ì— ë”°ë¼ ì‚¬ìš©í•  ì‹ ê²½ë§ í´ë˜ìŠ¤ ë‹¬ë¼ì§
                discount_factor=0.9, num_epoches=1000,
                ### ğŸ“¢balance ê³ ë ¤ 
                balance=100000000,
                start_epsilon=1, #balance.. RLTraderì—ì„œ ì‹ ìš©ê±°ë˜ì™€ ê°™ì´ ë³´ìœ  í˜„ê¸ˆ ë„˜ì–´ì„œëŠ” íˆ¬ìëŠ” ê³ ë ¤í•˜ì§€ ì•ŠìŒ. ë³´ìœ  í˜„ê¸ˆì´ ë¶€ì¡±í•˜ë©´ ì •ì±… ì‹ ê²½ë§ ê²°ê³¼ ë§¤ìˆ˜ê°€ ì¢‹ì•„ë„ ê´€ë§í•¨.
                #start_epsilonì€ ì´ˆê¸°íƒí—˜ ë¹„ìœ¨ì„ ë§í•˜ëŠ”ë°, ì „í˜€ í•™ìŠµë˜ì§€ ì•Šì€ ì´ˆê¸°ì—ëŠ” íƒí—˜ë¹„ìœ¨ í¬ê²Œ í•´ì„œ ë¬´ì‘ìœ„ íˆ¬ì í•˜ê²Œ í•´ì•¼í•¨. 
                value_network=None, policy_network=None,
                output_path='', reuse_models=True):
        # ì¸ì í™•ì¸
        assert min_trading_price > 0
        assert max_trading_price > 0
        assert max_trading_price >= min_trading_price
        assert num_steps > 0
        assert lr > 0
        # ê°•í™”í•™ìŠµ ì„¤ì •
        self.rl_method = rl_method
        self.discount_factor = discount_factor
        self.num_epoches = num_epoches
        self.start_epsilon = start_epsilon
        # í™˜ê²½ ì„¤ì •
        self.stock_code = stock_code
        self.chart_data = chart_data
        self.environment = Environment(chart_data) #í™˜ê²½ì€ ì°¨íŠ¸ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ ì½ìœ¼ë©´ì„œ ì£¼ê°€, ê±°ë˜ëŸ‰ ë“±ì˜ ë°ì´í„°ë¥¼ ì œê³µí•¨. 
        # ì—ì´ì „íŠ¸ ì„¤ì •
        self.agent = Agent(self.environment, balance, min_trading_price, max_trading_price) #ê°•í™”í•™ìŠµ í™˜ê²½ì„ ì¸ìë¡œ Agent í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•¨.
       
        # í•™ìŠµ ë°ì´í„°
        self.training_data = training_data
        self.sample = None
        self.training_data_idx = -1
        # ë²¡í„° í¬ê¸° = í•™ìŠµ ë°ì´í„° ë²¡í„° í¬ê¸° + ì—ì´ì „íŠ¸ ìƒíƒœ í¬ê¸° (47ê°œì˜ íŠ¹ì§• + ì—ì´ì „íŠ¸ì˜ ìƒíƒœ 3ê°œ(ë§¤ìˆ˜,ë§¤ë„,ê´€ë§)ë¥¼ ë”í•´ì„œ 50ê°œ!)
        self.num_features = self.agent.STATE_DIM
        if self.training_data is not None:
            self.num_features += self.training_data.shape[1]
        # ì‹ ê²½ë§ ì„¤ì •
        self.net = net
        self.num_steps = num_steps
        self.lr = lr
        self.value_network = value_network
        self.policy_network = policy_network
        self.reuse_models = reuse_models
        # ê°€ì‹œí™” ëª¨ë“ˆ
        self.visualizer = Visualizer()
        
        # ë©”ëª¨ë¦¬ (ê°•í™”í•™ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ê°ì¢… ë°ì´í„° ìŒ“ì•„ë‘ê¸° ìœ„í•´)
        self.memory_sample = []
        self.memory_action = []
        self.memory_reward = []
        self.memory_value = []
        self.memory_policy = []
        self.memory_pv = []
        self.memory_num_stocks = []
        self.memory_exp_idx = [] # íƒí—˜ ìœ„ì¹˜
        # ì´ë ‡ê²Œ ì €ì¥í•œ ìƒ˜í”Œ, ë³´ìƒ ë“±ì˜ ë°ì´í„°ë¡œ í•™ìŠµì„ ì§„í–‰í•¨
        # ì—í¬í¬ ê´€ë ¨ ì •ë³´
        self.loss = 0. # ë°œìƒí•œ ì†ì‹¤
        self.itr_cnt = 0 # ìˆ˜ìµ ë°œìƒ íšŸìˆ˜
        self.exploration_cnt = 0
        self.batch_size = 0
        # ë¡œê·¸ ë“± ì¶œë ¥ ê²½ë¡œ
        self.output_path = output_path
        
    # ê°€ì¹˜ ì‹ ê²½ë§ ìƒì„± í•¨ìˆ˜ -> netì— ì§€ì •ëœ ì‹ ê²½ë§ ì¢…ë¥˜ì— ë§ê²Œ ê°€ì¹˜ ì‹ ê²½ë§ ìƒì„±í•¨. -> ê°€ì¹˜ ì‹ ê²½ë§ì€ ì†ìµë¥ ì„ íšŒê·€ë¶„ì„í•˜ëŠ” ëª¨ë¸ 
    def init_value_network(self, shared_network=None, activation='linear', loss='mse'): #íšŒê·€ë¶„ì„ì´ë¼ activationì€ ì„ í˜•í•¨ìˆ˜ë¡œ, ì†ì‹¤í•¨ìˆ˜ëŠ” MSEë¡œ..
        if self.net == 'dnn':
            self.value_network = DNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, shared_network=shared_network,
                activation=activation, loss=loss)
        elif self.net == 'lstm':
            self.value_network = LSTMNetwork(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=activation, loss=loss)
        elif self.net == 'cnn':
            self.value_network = CNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=activation, loss=loss)
        if self.reuse_models and os.path.exists(self.value_network_path):
            self.value_network.load_model(model_path=self.value_network_path)

    # ì •ì±… ì‹ ê²½ë§ ìƒì„± í•¨ìˆ˜
    def init_policy_network(self, shared_network=None, activation='sigmoid', #ì–œ activationìœ¼ë¡œ sigmoid ì”€! -> ìƒ˜í”Œì— ëŒ€í•´ PVë¥¼ ë†’ì´ê¸° ìœ„í•´ ì·¨í•˜ê¸° ì¢‹ì€ í–‰ë™ì— ëŒ€í•œ ë¶„ë¥˜ ëª¨ë¸.
                            loss='binary_crossentropy'):
        if self.net == 'dnn':
            self.policy_network = DNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, shared_network=shared_network,
                activation=activation, loss=loss)
        elif self.net == 'lstm':
            self.policy_network = LSTMNetwork(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=activation, loss=loss)
        elif self.net == 'cnn':
            self.policy_network = CNN(
                input_dim=self.num_features, 
                output_dim=self.agent.NUM_ACTIONS, 
                lr=self.lr, num_steps=self.num_steps, 
                shared_network=shared_network,
                activation=activation, loss=loss)
        if self.reuse_models and os.path.exists(self.policy_network_path):
            self.policy_network.load_model(model_path=self.policy_network_path)

            
    # ì—í¬í¬ ì´ˆê¸°í™” í•¨ìˆ˜ -> ì—í¬í¬ë§ˆë‹¤ ìƒˆë¡œ ë°ì´í„°ê°€ ìŒ“ì´ëŠ” ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•˜ëŠ” reset() í•¨ìˆ˜.
    def reset(self):
        self.sample = None #ì½ì–´ì˜¨ ë°ì´í„°ëŠ” sampleì— ì €ì¥ë˜ëŠ”ë° ì´ˆê¸°í™” ë‹¨ê³„ì—ëŠ” ì½ì–´ì˜¨ í•™ìŠµë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ None
        self.training_data_idx = -1 # í•™ìŠµ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì²˜ìŒë¶€í„° ì½ê¸° ìœ„í•´ ì´ê±¸ -1ë¡œ ì¬ì„¤ì •í•¨. -> í•™ìŠµ ë°ì´í„° ì½ì–´ê°€ë©° 1ì”© ì¦ê°€.
        # í™˜ê²½ ì´ˆê¸°í™”
        self.environment.reset()
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.agent.reset()
        # ê°€ì‹œí™” ì´ˆê¸°í™”
        self.visualizer.clear([0, len(self.chart_data)])
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory_sample = [] #ì½ì–´ì˜¨ ë°ì´í„°ëŠ” sampleì— ì €ì¥ë¨. 
        self.memory_action = []
        self.memory_reward = []
        self.memory_value = []
        self.memory_policy = []
        self.memory_pv = []
        self.memory_num_stocks = []
        self.memory_exp_idx = []
        # ì—í¬í¬ ê´€ë ¨ ì •ë³´ ì´ˆê¸°í™”
        self.loss = 0.
        self.itr_cnt = 0 # ìˆ˜í–‰í•œ ì—í¬í¬ ìˆ˜ ì €ì¥.
        self.exploration_cnt = 0 # ë¬´ì‘ìœ„ íˆ¬ìë¥¼ ìˆ˜í–‰í•œ íšŸìˆ˜ ì €ì¥ (epsilon 0.1ì´ê³  100ë²ˆì˜ íˆ¬ì ê²°ì • ìˆìœ¼ë©´ 10ë²ˆ ë¬´ì‘ìœ„ íˆ¬ìí•¨.)
        self.batch_size = 0

    def build_sample(self): #í•™ìŠµë°ì´í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ìƒ˜í”Œ í•˜ë‚˜ ìƒì„±
        self.environment.observe() # í™˜ê²½ ê°ì²´ì˜ observe() í•¨ìˆ˜ í˜¸ì¶œí•´ì„œ ì°¨íŠ¸ ë°ì´í„°ì˜ í˜„ì¬ ì¸ë±ìŠ¤ì—ì„œ ë‹¤ìŒ ì¸ë±ìŠ¤ ë°ì´í„°ë¥¼ ì½ê²Œí•¨. 
        if len(self.training_data) > self.training_data_idx + 1: # ê·¸ë¦¬ê³  í•™ìŠµ ë°ì´í„°ì˜ ë‹¤ìŒ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ 
            self.training_data_idx += 1 # -> í•™ìŠµ ë°ì´í„°ì— ë‹¤ìŒ ì¸ë±ìŠ¤ ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ ë³€ìˆ˜ 1ë§Œí¼ ì¦ê°€ì‹œí‚¤ê³ , 
            self.sample = self.training_data.iloc[self.training_data_idx].tolist() # training data ë°°ì—´ì—ì„œ idx ì¸ë±ìŠ¤ ë°ì´í„° ë°›ì•„ì™€ì„œ sampleì— ì €ì¥í•¨. (v3ëŠ” 47ê°œë¡œ êµ¬ì„±. )
            self.sample.extend(self.agent.get_states())  # sampleì— ì—ì´ì „íŠ¸ ìƒíƒœ ì¶”ê°€í•´ sampleì˜ 50ê°œ ê°’ìœ¼ë¡œ êµ¬ì„±.
            return self.sample
        return None

    @abc.abstractmethod # ì¶”ìƒ ë©”ì„œë“œ.. í•˜ìœ„ í´ë˜ìŠ¤ë“¤ì€ ë°˜ë“œì‹œ ì´ í•¨ìˆ˜ êµ¬í˜„í•´ì•¼í•¨.ReinforcementLearner í´ë˜ìŠ¤ ìƒì†í•˜ê³ ë„ ì¶”ìƒ ë©”ì„œë“œ êµ¬í˜„ ì•ˆí•˜ë©´ NotImplemented ì˜ˆì™¸ê°€ ë°œìƒí•¨. 
    def get_batch(self): #ì‹ ê²½ë§ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë°°ì¹˜ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±
        pass

    # ì‹ ê²½ë§ì„ í•™ìŠµí•˜ëŠ” fit() í•¨ìˆ˜ ë³´ì—¬ì¤Œ. 
    def fit(self):
        # ë°°ì¹˜ í•™ìŠµ ë°ì´í„° ìƒì„±
        x, y_value, y_policy = self.get_batch()
        # ì†ì‹¤ ì´ˆê¸°í™”
        self.loss = None
        if len(x) > 0:
            loss = 0
            if y_value is not None:
                # ê°€ì¹˜ ì‹ ê²½ë§ ê°±ì‹ 
                loss += self.value_network.train_on_batch(x, y_value) # ê°€ì¹˜ ì‹ ê²½ë§ í•™ìŠµí•˜ê¸° ìœ„í•´. DQNLearner, ActorCriticLearner, A2CLearnerì—ì„œ í•™ìŠµ.
            if y_policy is not None:
                # ì •ì±… ì‹ ê²½ë§ ê°±ì‹ 
                loss += self.policy_network.train_on_batch(x, y_policy) # ì •ì±… ì‹ ê²½ë§ í•™ìŠµí•˜ê¸° ìœ„í•´. PolicyGradientLearner, ActorCriticLearner, A2cLearnerì—ì„œ í•™ìŠµí•¨. 
            self.loss = loss # í•™ìŠµ í›„ ë°œìƒí•˜ëŠ” ì†ì‹¤ì„ ì¸ìŠ¤í„´ìŠ¤ ì†ì„±ìœ¼ë¡œ ì €ì¥í•¨. ê°€ì¹˜ ì‹ ê²½ë§ê³¼ ì •ì±… ì‹ ê²½ë§ í•™ìŠµí•˜ëŠ” ê²½ìš° ë‘ í•™ìŠµ ì†ì‹¤ì„ í•©ì‚°í•´ ë°˜í™˜í•¨. 

    # í•˜ë‚˜ì˜ ì—í¬í¬ê°€ ì™„ë£Œë˜ì–´ ì—í¬í¬ ê´€ë ¨ ì •ë³´ ê°€ì‹œí™”í•˜ëŠ” ë¶€ë¶„ 
    def visualize(self, epoch_str, num_epoches, epsilon):
        self.memory_action = [Agent.ACTION_HOLD] * (self.num_steps - 1) + self.memory_action
        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks
        if self.value_network is not None: #LSTMê³¼ CNN ì‹ ê²½ë§ ì‚¬ìš©í•˜ë©´ ì—ì´ì „íŠ¸ í–‰ë™, ë³´ìœ  ì£¼ì‹ ìˆ˜ ë“±ë“± í™˜ê²½ì˜ ì¼ë´‰ìˆ˜ë³´ë‹¤ (num_steps - 1) ë§Œí¼ ë¶€ì¡±í•˜ë¯€ë¡œ ì˜ë¯¸ ì—†ëŠ”ê°’ì„ ì²«ë¶€ë¶„ì— ì±„ì›Œì¤Œ
            self.memory_value = [np.array([np.nan] * len(Agent.ACTIONS))] \
                                * (self.num_steps - 1) + self.memory_value
        if self.policy_network is not None:
            self.memory_policy = [np.array([np.nan] * len(Agent.ACTIONS))] \
                                * (self.num_steps - 1) + self.memory_policy
        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv 
       
        # ê°€ì‹œí™” ì‹œì‘
        self.visualizer.plot(
            epoch_str=epoch_str, num_epoches=num_epoches, 
            epsilon=epsilon, action_list=Agent.ACTIONS,  #ì—ì´ì „íŠ¸ í–‰ë™, ë³´ìœ  ì£¼ì‹ìˆ˜, ê°€ì¹˜ì‹ ê²½ë§ì¶œë ¥, ì •ì±…ì‹ ê²½ë§ì¶œë ¥, í¬í´ê°€ì¹˜, íƒí—˜ ìœ„ì¹˜ ë“±..
            actions=self.memory_action, 
            num_stocks=self.memory_num_stocks, 
            outvals_value=self.memory_value, 
            outvals_policy=self.memory_policy,
            exps=self.memory_exp_idx, 
            initial_balance=self.agent.initial_balance, 
            pvs=self.memory_pv,
        )
        self.visualizer.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))

    def run(self, learning=True): #í•™ìŠµí•´ì„œ ì‹ ê²½ë§ ëª¨ë¸ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´ learningì„ trueë¡œ, í•™ìŠµëœ ëª¨ë¸ ê°€ì§€ê³  íˆ¬ì ì‹œë®¬ë ˆì´ì…˜ë§Œ í•˜ë ¤ë©´ learningì„ falseë¡œ.
        info = (
            f'[{self.stock_code}] RL:{self.rl_method} NET:{self.net} '
            f'LR:{self.lr} DF:{self.discount_factor} '
        )
        with self.lock:
            logger.debug(info) #ê°•í™” í•™ìŠµ ì„¤ì •ì„ ë¡œê·¸ë¡œ ê¸°ë¡í•¨.

        # ì‹œì‘ ì‹œê°„
        time_start = time.time()

        # ê°€ì‹œí™” ì¤€ë¹„
        # ì°¨íŠ¸ ë°ì´í„°ëŠ” ë³€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¯¸ë¦¬ ê°€ì‹œí™”
        self.visualizer.prepare(self.environment.chart_data, info) #ê°€ì‹œí™” ì¤€ë¹„ -> ì—í¬í¬ê°€ ì§„í–‰ë¼ë„ ë³€í•˜ì§€ ì•ŠëŠ” ì£¼ì‹íˆ¬ì í™˜ê²½ì¸ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ê°€ì‹œí™”í•¨. 

        # ê°€ì‹œí™” ê²°ê³¼ ì €ì¥í•  í´ë” ì¤€ë¹„ -> ê²°ê³¼ëŠ” output_path ê²½ë¡œ í•˜ìœ„ì˜ epoch_summary_* í´ë”ì— ì €ì¥ë¨. 
        self.epoch_summary_dir = os.path.join(self.output_path, f'epoch_summary_{self.stock_code}')
        if not os.path.isdir(self.epoch_summary_dir):
            os.makedirs(self.epoch_summary_dir)
        else:
            for f in os.listdir(self.epoch_summary_dir):
                os.remove(os.path.join(self.epoch_summary_dir, f))

        # í•™ìŠµì— ëŒ€í•œ ì •ë³´ ì´ˆê¸°í™”
        max_portfolio_value = 0 #ì—¬ê¸°ì—” ìˆ˜í–‰í•œ ì—í¬í¬ ì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ê°€ ì €ì¥ë¨.
        epoch_win_cnt = 0 # ì—¬ê¸°ì—” ìˆ˜í–‰í•œ ì—í¬í¬ ì¤‘ì—ì„œ ìˆ˜ìµì´ ë°œìƒí•œ ì—í¬í¬ ìˆ˜ë¥¼ ì €ì¥í•¨. (í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ê°€ ì´ˆê¸° ìë³¸ê¸ˆë³´ë‹¤ ë†’ì•„ì§„ ì—í¬í¬ ìˆ˜)

        # ì—í¬í¬ ë°˜ë³µ #-> ì§€ì •ëœ ì—í¬í¬ ìˆ˜ë§Œí¼ ì£¼ì‹íˆ¬ì ì‹œë®¬ë ˆì´ì…˜ ë°˜ë³µí•˜ë©° í•™ìŠµ
        for epoch in tqdm(range(self.num_epoches)):
            time_start_epoch = time.time()

            # step ìƒ˜í”Œì„ ë§Œë“¤ê¸° ìœ„í•œ í (num_stepë§Œí¼ ìƒ˜í”Œì„ ë‹´ì•„ë‘˜ í ì´ˆê¸°í™”)
            q_sample = collections.deque(maxlen=self.num_steps)
            
            # í™˜ê²½, ì—ì´ì „íŠ¸, ì‹ ê²½ë§, ê°€ì‹œí™”, ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            self.reset()

            # í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ë¡ íƒí—˜ ë¹„ìœ¨ ê°ì†Œ -> epsilon ê°’ ì •í•  ë•ŒëŠ” ìµœì´ˆ ë¬´ì‘ìœ„ íˆ¬ì ë¹„ìœ¨ì¸ start_epsilon ê°’ì— í˜„ì¬ epoch ìˆ˜ì— í•™ìŠµì§„í–‰ë¥ ì„ ê³±í•´ì„œ ì •í•¨. 
            if learning:
                epsilon = self.start_epsilon * (1 - (epoch / (self.num_epoches - 1)))
            else:
                epsilon = self.start_epsilon

            # í•™ìŠµ ë°ì´í„° ë°˜ë³µì˜ ì´ˆë°˜ë¶€
            for i in tqdm(range(len(self.training_data)), leave=False):
                # ìƒ˜í”Œ ìƒì„±
                next_sample = self.build_sample() #build_sample() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ í™˜ê²½ ê°ì²´ë¡œë¶€í„° í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ì½ì–´ì˜´. 
                if next_sample is None: # next_sampleì´ Noneì´ë¼ë©´ ë§ˆì§€ë§‰ê¹Œì§€ ë°ì´í„°ë¥¼ ë‹¤ ì½ì€ ê²ƒì´ë¯€ë¡œ í•™ìŠµ ë°ì´í„° ë°˜ë³µì„ ì¢…ë£Œí•¨. 
                    break
                
                ### *âš ï¸ì½”ë“œ ìˆ˜ì • ì‹œì‘âš ï¸*  ###
                # ë§ˆì§€ë§‰ ë°ì´í„° í¬ì¸íŠ¸ í™•ì¸
                is_last_step = (i == len(self.training_data) - 1)
                # num_stepsë§Œí¼ ìƒ˜í”Œ ì €ì¥ 
                q_sample.append(next_sample)
                if len(q_sample) < self.num_steps and not is_last_step: #num_steps ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œì´ ì¤€ë¹„ë¼ì•¼ í–‰ë™ì„ ê²°ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒ˜í”Œ íì— ìƒ˜í”Œì´ ëª¨ë‘ ì°° ë•Œê¹Œì§€ continueë¥¼ í†µí•´ ì´í›„ ë¡œì§ì„ ê±´ë„ˆëœ€.
                    continue
                          
                # ë§ˆì§€ë§‰ ìŠ¤í…ì´ë©´ ëª¨ë“  ì£¼ì‹ ë§¤ë„
                if is_last_step:
                    action = Agent.ACTION_SELL  # ë§¤ë„ í–‰ë™ ê°•ì œ ì„¤ì •
                    confidence = 1.0  # ë§¤ë„ì— ëŒ€í•œ í™•ì‹  100%
                    reward = self.agent.act(action, confidence)  # ë§¤ë„ ì‹¤í–‰
                    exploration = False  # íƒí—˜ ì—†ìŒ
                    
                else:
                    # ì‹ ê²½ë§ ì˜ˆì¸¡ ë° í–‰ë™ ê²°ì • ë¡œì§
                    # ê°€ì¹˜ ì‹ ê²½ë§ê³¼ ì •ì±… ì‹ ê²½ë§ìœ¼ë¡œ ì˜ˆì¸¡ í–‰ë™ ê°€ì¹˜ì™€ ì˜ˆì¸¡ í–‰ë™ í™•ë¥ ì„ êµ¬í•˜ëŠ” ë¶€ë¶„
                    # ê°€ì¹˜, ì •ì±… ì‹ ê²½ë§ ì˜ˆì¸¡
                    pred_value, pred_policy = None, None
                    if self.value_network:
                        pred_value = self.value_network.predict(list(q_sample))
                    if self.policy_network:
                        pred_policy = self.policy_network.predict(list(q_sample))
                    # ì‹ ê²½ë§ ë˜ëŠ” íƒí—˜ì— ì˜í•œ í–‰ë™ ê²°ì • (ìœ„ì—ì„œ êµ¬í•œ ì˜ˆì¸¡ ê°€ì¹˜ì™€ í™•ë¥ ë¡œ íˆ¬ì í–‰ë™ì„ ê²°ì •í•¨. -> ì—¬ê¸°ì„  ë§¤ìˆ˜ì™€ ë§¤ë„ ì¤‘ í•˜ë‚˜ë¥¼ ê²°ì •í•¨)
                    action, confidence, exploration = self.agent.decide_action(pred_value, pred_policy, epsilon)
                    # decide_action() í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” ê°’ì€ ì„¸ ê°€ì§€ì„. -> ê²°ì •í•œ í–‰ë™ì¸ action, ê²°ì •ì— ëŒ€í•œ í™•ì‹ ë„ì¸ confidence, ë¬´ì‘ìœ„ ì¶”ì ìœ ë¬´ì¸ exploration
                   
                    # ê²°ì •í•œ í–‰ë™ì„ ìˆ˜í–‰í•˜ê³  ë³´ìƒ íšë“
                    reward = self.agent.act(action, confidence) # ê²°ì •í•œ í–‰ë™ì„ ìˆ˜í–‰í•˜ë„ë¡ ì—ì´ì „íŠ¸ì˜ act() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•¨. act() í•¨ìˆ˜ëŠ” í–‰ë™ì„ ìˆ˜í–‰í•˜ê³  í–‰ë™ ìˆ˜í–‰ í›„ì˜ ì†ìµ(reward)ì„ ë°˜í™˜í•¨.
                    ### *âš ï¸ì½”ë“œ ìˆ˜ì • ëâš ï¸*  ###
                    
                    
                # í–‰ë™ ë° í–‰ë™ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ê¸°ì–µ -> ë©”ëª¨ë¦¬ ë³€ìˆ˜ë“¤ì€ 1) í•™ìŠµì—ì„œ ë°°ì¹˜ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš© 2) ê°€ì‹œí™”ê¸°ì—ì„œ ì°¨íŠ¸ ê·¸ë¦´ ë•Œ ì‚¬ìš©
                self.memory_sample.append(list(q_sample))
                self.memory_action.append(action)
                self.memory_reward.append(reward)
                if self.value_network is not None:
                    self.memory_value.append(pred_value)
                if self.policy_network is not None:
                    self.memory_policy.append(pred_policy)
                self.memory_pv.append(self.agent.portfolio_value)
                self.memory_num_stocks.append(self.agent.num_stocks)
                if exploration:
                    self.memory_exp_idx.append(self.training_data_idx)

                # ë°˜ë³µì— ëŒ€í•œ ì •ë³´ ê°±ì‹ 
                self.batch_size += 1 # ë°°ì¹˜ í¬ê¸°
                self.itr_cnt += 1 # ë°˜ë³µ ì¹´ìš´íŒ… íšŸìˆ˜
                self.exploration_cnt += 1 if exploration else 0 # ë¬´ì‘ìœ„ íˆ¬ì íšŸìˆ˜(íƒí—˜í•œ ê²½ìš°ì—ë§Œ 1 ì¦ê°€ì‹œí‚¤ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0 ë”í•´ì„œ ë³€í™” ì—†ê²Œ.)

            # ì—í¬í¬ ì¢…ë£Œ í›„ í•™ìŠµ
            if learning: # ë” ì´ìƒ ìƒ˜í”Œì´ ì—†ëŠ” ê²½ìš° for ë¸”ë¡ì„ ë¹ ì ¸ë‚˜ì˜´.
                self.fit() # í•™ìŠµ ëª¨ë“œì¸ ê²½ìš° for ë¸”ë¡ì„ ë¹ ì ¸ë‚˜ì˜¨ í›„ì— ì‹ ê²½ë§ í•™ìŠµ í•¨ìˆ˜ì¸ fit() í˜¸ì¶œí•¨. 

                
            # ì—í¬í¬ ê´€ë ¨ ì •ë³´ ë¡œê·¸ ê¸°ë¡(ì •ë³´ ë¡œê¹…í•˜ê³  ê°€ì‹œí™”í•˜ëŠ” ë¶€ë¶„)
            num_epoches_digit = len(str(self.num_epoches)) #ì´ ì—í¬í¬ ìˆ˜ì˜ ë¬¸ìì—´ ê¸¸ì´ í™•ì¸ (ì´ ì—í¬í¬ ìˆ˜ê°€ 1,000ì´ë©´ ê¸¸ì´ëŠ” 4ê°€ë¨. )
            epoch_str = str(epoch + 1).rjust(num_epoches_digit, '0') # í˜„ì¬ ì—í¬í¬ ìˆ˜ë¥¼ num_epochs_digit ê¸¸ì´ì˜ ë¬¸ìì—´ë¡œ ë§Œë“¤ì–´ epoch_strì— ì €ì¥í•¨. ex) ë„¤ ìë¦¬ ë¬¸ìì—´ ë§Œë“ ë‹¤ í•  ë•Œ, ì²«ë²ˆì§¸ ì—í¬í¬ì˜ ê²½ìš° epcohê°€ 0ì´ë¯€ë¡œ 1ì„ ë”í•˜ê³  ì•ì— 0ì„ ì±„ì›Œì„œ 0001ë¡œ.
            time_end_epoch = time.time() # í˜„ì¬ ì‹œê°„
            elapsed_time_epoch = time_end_epoch - time_start_epoch #í˜„ì¬ ì‹œê°„ì—ì„œ ì‹œì‘ì‹œê°„ ë¹¼ì„œ ì—í¬í¬ ìˆ˜í–‰ ì†Œìš” ì‹œê°„ì„ ì €ì¥í•¨.
            logger.debug(f'[{self.stock_code}][Epoch {epoch_str}/{self.num_epoches}] '
                f'Epsilon:{epsilon:.4f} #Expl.:{self.exploration_cnt}/{self.itr_cnt} '
                f'#Buy:{self.agent.num_buy} #Sell:{self.agent.num_sell} #Hold:{self.agent.num_hold} '
                f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '
                f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}')

            # ì—í¬í¬ ê´€ë ¨ ì •ë³´ ê°€ì‹œí™”
            if self.num_epoches == 1 or (epoch + 1) % int(self.num_epoches / 10) == 0:
                self.visualize(epoch_str, self.num_epoches, epsilon) #ê°€ì‹œí™”ê¸° ê°ì²´ë¥¼ ì´ìš©í•´ ì—í¬í¬ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ê·¸ë¦¼ìœ¼ë¡œ ê°€ì‹œí™”í•œ í›„ íŒŒì¼ë¡œ ì €ì¥í•¨

            # í•™ìŠµ ê´€ë ¨ ì •ë³´ ê°±ì‹ 
            max_portfolio_value = max(
                max_portfolio_value, self.agent.portfolio_value) #ì—í¬í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë™ì•ˆ ìµœëŒ€ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ë¥¼ ê°±ì‹ í•˜ê³ 
            if self.agent.portfolio_value > self.agent.initial_balance:
                epoch_win_cnt += 1  #í•´ë‹¹ ì—í¬í¬ì—ì„œ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ê°€ ìë³¸ê¸ˆë³´ë‹¤ ë†’ìœ¼ë©´ epcch_win_cntë¥¼ ì¦ê°€ì‹œí‚´. 

        # ì¢…ë£Œ ì‹œê°„
        time_end = time.time()
        elapsed_time = time_end - time_start # ì „ì²´ ì—í¬í¬ ìˆ˜í–‰ ì†Œìš” ì‹œê°„ ê¸°ë¡

        # í•™ìŠµ ê´€ë ¨ ì •ë³´ ë¡œê·¸ ê¸°ë¡
        with self.lock:
            logger.debug(f'[{self.stock_code}] Elapsed Time:{elapsed_time:.4f} ' #ì „ì²´ì†Œìš”ì‹œê°„, ìµœëŒ€ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜, í¬í´ê°€ì¹˜ê°€ ìë³¸ê¸ˆë³´ë‹¤ ë†’ì•˜ë˜ ì—í¬í¬ ìˆ˜ë¥¼ ë¡œê·¸ë¡œ ë‚¨ê¹€.
                f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}')